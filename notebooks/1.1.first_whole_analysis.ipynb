{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all src modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cProfile\n",
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    import cld3\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "import pycld2\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import datetime\n",
    "import src.utils.geometry as geo\n",
    "import src.utils.places_to_cells as places_to_cells\n",
    "import src.utils.join_and_count as join_and_count\n",
    "import src.utils.make_config as make_config\n",
    "import src.data.shp_extract as shp_extract\n",
    "import src.data.text_process as text_process\n",
    "import src.data.access as data_access\n",
    "import src.data.user_filters as ufilters\n",
    "import src.data.user_agg as uagg\n",
    "import src.data.metrics as metrics\n",
    "import src.visualization.grid_viz as grid_viz\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container </style>\"))\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = os.environ['DATA_DIR']\n",
    "tweets_files_format = 'tweets_{}_{}_{}.json.gz'\n",
    "places_files_format = 'places_{}_{}_{}.json.gz'\n",
    "ssh_domain = os.environ['IFISC_DOMAIN']\n",
    "ssh_username = os.environ['IFISC_USERNAME']\n",
    "project_data_dir = os.path.join('..', 'data')\n",
    "external_data_dir = os.path.join(project_data_dir, 'external')\n",
    "interim_data_dir = os.path.join(project_data_dir, 'interim')\n",
    "processed_data_dir = os.path.join(project_data_dir, 'processed')\n",
    "cell_data_path_format = os.path.join(processed_data_dir,\n",
    "                                     '{}_cell_data_cc={}_cell_size={}m.geojson')\n",
    "latlon_proj = 'epsg:4326'\n",
    "LANGS_DICT = dict([(lang[1],lang[0].lower().capitalize())\n",
    "                   for lang in pycld2.LANGUAGES])\n",
    "\n",
    "country_codes = ('BE', 'BO', 'CA', 'CH', 'EE', 'ES', 'FR', 'HK', 'ID', 'LT', \n",
    "                 'LV', 'MY', 'PE', 'RO', 'SG', 'TN', 'UA')\n",
    "with open(os.path.join(external_data_dir, 'countries.json')) as f:\n",
    "    countries_study_data = json.load(f)\n",
    "with open(os.path.join(external_data_dir, 'langs_agg.json')) as f:\n",
    "    langs_agg_dict = json.load(f)\n",
    "\n",
    "# Country-specific parameters\n",
    "cc = 'BE'\n",
    "region = None\n",
    "# region = 'CataluÃ±a'\n",
    "if region:\n",
    "    area_dict = countries_study_data[cc]['regions'][region]\n",
    "else:\n",
    "    area_dict = countries_study_data[cc]\n",
    "    \n",
    "fig_dir = os.path.join('..', 'reports', 'figures', cc)\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(os.path.join(fig_dir, 'counts'))\n",
    "    os.makedirs(os.path.join(fig_dir, 'prop'))\n",
    "xy_proj = area_dict['xy_proj']\n",
    "cc_timezone = area_dict['timezone']\n",
    "plot_langs_list = area_dict['local_langs']\n",
    "min_poly_area = area_dict.get('min_poly_area') or 0.1\n",
    "max_place_area = area_dict.get('max_place_area') or 1e9 # linked to cell size and places data\n",
    "valid_uids_path = os.path.join(interim_data_dir, f'valid_uids_{cc}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of bots, company account (eg careerarc, tweetmyjobs). If can't distinguish companies by source like careerarc, then how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places, area and grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'])\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "shape_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Places can be a point too -> treat them like tweets with coords in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_files_paths = [\n",
    "    os.path.join(data_dir_path, places_files_format.format(2015, 2018, cc))]\n",
    "#     os.path.join(data_dir_path, places_files_format.format(2019, 2019, cc))]\n",
    "all_raw_places_df = []\n",
    "for file in places_files_paths:\n",
    "    raw_places_df = data_access.return_json(file,\n",
    "        ssh_domain=ssh_domain, ssh_username=ssh_username, compression='gzip')\n",
    "    all_raw_places_df.append(\n",
    "        raw_places_df[['id', 'bounding_box', 'name', 'place_type']])\n",
    "\n",
    "# We drop the duplicate places (based on their ID)\n",
    "places_df = pd.concat(all_raw_places_df).drop_duplicates(subset='id')\n",
    "places_geodf, places_in_xy = geo.make_places_geodf(places_df, shape_df,\n",
    "                                                   xy_proj=xy_proj)\n",
    "places_geodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_geodf.loc[places_geodf['name'] == 'Brussels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = 10000\n",
    "cells_df, cells_in_area_df, Nx, Ny = geo.create_grid(\n",
    "    shape_df, cell_size, xy_proj=xy_proj, intersect=True)\n",
    "grid_test_df = cells_in_area_df.copy()\n",
    "grid_test_df['metric'] = 1\n",
    "save_path = os.path.join(fig_dir, f'grid_cc={cc}_cell_size={cell_size}m.pdf')\n",
    "plot_kwargs = dict(alpha=0.7, edgecolor='w', linewidths=0.5, cmap='plasma')\n",
    "ax = grid_viz.plot_grid(grid_test_df, shape_df, metric_col='metric', show=True, \n",
    "                        save_path=save_path, xy_proj=xy_proj, **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_files_paths = [\n",
    "    os.path.join(data_dir_path, tweets_files_format.format(2015, 2018, cc))]\n",
    "#     os.path.join(data_dir_path, tweets_files_format.format(2019, 2019, cc))]\n",
    "\n",
    "def read_data(tweets_file_path, chunk_start, chunk_size, places_geodf):\n",
    "    raw_tweets_df = data_access.read_json_wrapper(\n",
    "        tweets_file_path, chunk_start, chunk_size, ssh_domain=ssh_domain,\n",
    "        ssh_username=ssh_username)\n",
    "    og_cols = raw_tweets_df.columns.values\n",
    "    raw_tweets_df = raw_tweets_df.join(places_geodf, on='place_id', how='inner')\n",
    "    raw_tweets_df = raw_tweets_df.loc[:, og_cols]\n",
    "    return raw_tweets_df\n",
    "\n",
    "def profile_pre_process(tweets_file_path, chunk_start, chunk_size):\n",
    "    cProfile.runctx(\n",
    "        'read_data(tweets_file_path, chunk_start, chunk_size, places_geodf)', \n",
    "        globals(), locals())\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    tweets_access_res = []\n",
    "    for file_path in tweets_files_paths:\n",
    "        for chunk_start, chunk_size in data_access.chunkify(\n",
    "                file_path, size=1e9, ssh_domain=ssh_domain, \n",
    "                ssh_username=ssh_username):\n",
    "            tweets_access_res.append(pool.apply_async(\n",
    "                read_data, (file_path, chunk_start, chunk_size, places_geodf)))\n",
    "    \n",
    "    # This is mandatory so that the pool doesn't stop working until every\n",
    "    # chunk has been processed.\n",
    "    for res in tweets_access_res:\n",
    "        res.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeted_months = None\n",
    "tweets_pb_months = None\n",
    "first_day = datetime.datetime(year=2015, month=1, day=1)\n",
    "for res in tweets_access_res:\n",
    "    tweets_df = res.get().copy()\n",
    "    tweets_df = tweets_df.loc[tweets_df['created_at'] > first_day]\n",
    "    tweets_df['month'] = tweets_df['created_at'].dt.to_period('M')\n",
    "    has_gps = tweets_df['coordinates'].notnull()\n",
    "    geometry = tweets_df.loc[has_gps, 'coordinates'].apply(\n",
    "        lambda x: Point(x['coordinates']))\n",
    "    tweets_coords = geopd.GeoSeries(geometry, crs=latlon_proj, \n",
    "                                    index=tweets_df.loc[has_gps].index)\n",
    "    tweets_df = tweets_df.join(places_geodf, on='place_id', how='left')\n",
    "    coords_in_place = tweets_coords.within(\n",
    "        geopd.GeoSeries(tweets_df.loc[has_gps, 'geometry']))\n",
    "    \n",
    "    tweeted_months = join_and_count.increment_counts(\n",
    "        tweeted_months, tweets_df, ['month'])\n",
    "    tweets_pb_months = join_and_count.increment_counts(tweets_pb_months, \n",
    "        tweets_df.loc[has_gps].loc[~coords_in_place], ['month'])\n",
    "#     print(coords_in_place.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_counts = tweeted_months.join(tweets_pb_months, rsuffix='_pb', how='left')\n",
    "months_counts['prop'] = months_counts['count_pb'] / months_counts['count']\n",
    "ax = months_counts['prop'].plot.bar()\n",
    "ticks = np.arange(0,60,5)\n",
    "tick_labels = ax.get_xticklabels()\n",
    "_ = ax.set_xticks(ticks)\n",
    "_ = ax.set_xticklabels([tick_labels[i] for i in ticks])\n",
    "_ = ax.set_ylabel('proportion')\n",
    "_ = ax.set_title('Proportion of tweets with coords outside of place')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters: user-based imply a loop over all the raw_tweets_df, and must be applied before getting tweets_lang_df and even tweets_loc_df, because these don't interest us at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is incremental, so can't parallelize. And it's rather fast, so not worth the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeted_months_users = None\n",
    "for res in tweets_access_res:\n",
    "    raw_tweets_df = res.get()\n",
    "    nr_users = len(raw_tweets_df['uid'].unique())\n",
    "    print(f'There are {nr_users} distinct users in this chunk.')\n",
    "    tweeted_months_users = ufilters.inc_months_activity(\n",
    "        tweeted_months_users, raw_tweets_df)\n",
    "\n",
    "tweeted_months_users = tweeted_months_users['count']\n",
    "total_nr_users = len(tweeted_months_users.index.levels[0])\n",
    "print(f'In total, there are {total_nr_users} distinct users in the whole dataset.')\n",
    "local_uids = ufilters.consec_months(tweeted_months_users)\n",
    "bot_uids = ufilters.bot_activity(tweeted_months_users)\n",
    "# We have local_uids: index of uids with a column full of True, and bot_uids:\n",
    "# index of uids with a column full of False. When we multiply them, the uids\n",
    "# in local_uids which are not in bot_uids are assigned NaN, and the ones which \n",
    "# are in bot_uids are assigned False. When we convert to the boolean type,\n",
    "# the NaNs turn to True.\n",
    "valid_uids = (local_uids * bot_uids).astype('bool').rename('valid')\n",
    "valid_uids = valid_uids.loc[valid_uids]\n",
    "print(f'This leaves us with {len(valid_uids)} valid users in the whole dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_filter(raw_tweets_df, valid_uids, places_in_xy, max_distance):\n",
    "    tweets_df = raw_tweets_df.join(valid_uids, on='uid', how='inner')\n",
    "    too_fast_uids = ufilters.too_fast(tweets_df, places_in_xy, max_distance)\n",
    "    return too_fast_uids\n",
    "\n",
    "too_fast_uids_series = pd.Series([])\n",
    "area_bounds = shape_df.to_crs(xy_proj).geometry.iloc[0].bounds\n",
    "# Get an upper limit of the distance that can be travelled inside the area\n",
    "max_distance = np.sqrt((area_bounds[0]-area_bounds[2])**2 \n",
    "                       + (area_bounds[1]-area_bounds[3])**2)\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    cols = ['uid', 'created_at', 'place_id', 'coordinates']\n",
    "    map_parameters = [\n",
    "        (res.get().loc[:, cols], valid_uids, places_in_xy, max_distance) \n",
    "        for res in tweets_access_res]\n",
    "    print('entering the loop')\n",
    "    too_fast_uids_list = pool.starmap_async(speed_filter, map_parameters).get()\n",
    "    for too_fast_uids in too_fast_uids_list:\n",
    "        too_fast_uids_series = (too_fast_uids_series * too_fast_uids).fillna(False)\n",
    "\n",
    "print(f'In total, there are {len(too_fast_uids_series)} too fast users left to '\n",
    "      'filter out in the whole dataset.')\n",
    "valid_uids = (valid_uids * too_fast_uids_series).astype('bool').rename('valid')\n",
    "valid_uids = valid_uids.loc[valid_uids]\n",
    "print(f'This leaves us with {len(valid_uids)} valid users in the whole dataset.')\n",
    "valid_uids.index = valid_uids.index.rename('uid')\n",
    "valid_uids.to_csv(valid_uids_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most tweets in the month in that country to asign local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't filter out tweets with a useless place (one too large) here, because these tweets can still be useful for language detection. So this filter is only applied later on. Similarly, we keep tweets with insufficient text to make a reliable language detection, because they can still be useful for residence attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_uids = pd.read_csv(valid_uids_path, index_col='uid', header=0)\n",
    "\n",
    "def process(raw_tweets_df, valid_uids, places_geodf, langs_agg_dict, \n",
    "            text_col='text', min_nr_words=4, cld='pycld2'):\n",
    "    cols = ['text', 'id', 'lang', 'place_id', 'coordinates', 'uid', \n",
    "            'created_at', 'source']\n",
    "    tweets_loc_df = raw_tweets_df.loc[:, cols]\n",
    "    print('- starting geo join')\n",
    "    tweets_loc_df = tweets_loc_df.join(valid_uids, on='uid', how='inner')\n",
    "    has_gps = tweets_loc_df['coordinates'].notnull()\n",
    "    tweets_places_df = tweets_loc_df.loc[~has_gps].join(\n",
    "        places_geodf[['geometry', 'area']], on='place_id', how='left')\n",
    "    # The geometry of the tweets with GPS coordinates is the Point associated \n",
    "    # to them.\n",
    "    tweets_loc_df.loc[has_gps, 'geometry'] = tweets_loc_df.loc[has_gps, 'coordinates'].apply(\n",
    "        lambda x: Point(x['coordinates']))\n",
    "    # We assign the area of points to 0, and at the same time initialize the \n",
    "    # whole column, whose values will change for tweets without GPS coordinates.\n",
    "    tweets_loc_df['area'] = 0\n",
    "    # We add the geometry of the place to the tweets without GPS coordinates\n",
    "    tweets_loc_df.loc[~has_gps, 'geometry'] = tweets_places_df['geometry']\n",
    "    tweets_loc_df.loc[~has_gps, 'area'] = tweets_places_df['area']\n",
    "    tweets_loc_df = (tweets_loc_df.rename(columns={'lang': 'twitter_lang'})\n",
    "                                  .drop(columns=['valid', 'coordinates']))\n",
    "    tweets_loc_df = geopd.GeoDataFrame(tweets_loc_df, crs=latlon_proj)\n",
    "    print('starting lang detect')\n",
    "    tweets_lang_df = text_process.lang_detect(tweets_loc_df, text_col=text_col, \n",
    "        min_nr_words=min_nr_words, cld=cld, langs_agg_dict=langs_agg_dict)\n",
    "    print('chunk done')\n",
    "    return tweets_lang_df\n",
    "\n",
    "\n",
    "def profile_process(raw_tweets_df, valid_uids, places_geodf):\n",
    "    cProfile.runctx(\n",
    "        'process(raw_tweets_df, valid_uids, places_geodf)', globals(), locals())\n",
    "\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    map_parameters = [(res.get(), valid_uids, places_geodf, langs_agg_dict) \n",
    "                      for res in tweets_access_res]\n",
    "    print('entering the loop')\n",
    "    tweets_process_res = pool.starmap_async(process, map_parameters).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study at the tweet level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tweet counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_level_label = 'tweets in {}'\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, tweet_level_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why sjoin so slow? It tests on every cell, even though it's exclusive: if one cell matches no other will. Solution: loop over cells, ordered by the counts obtained from places, and stop at first match, will greatly reduce the number of 'within' operations -> update: doesn't seem possible, deleting from spatial index is extremely slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langs_counts(tweets_lang_df, max_place_area, cells_in_area_df):\n",
    "    tweets_df = tweets_lang_df.copy()\n",
    "    relevant_area_mask = tweets_df['area'] < max_place_area\n",
    "    tweets_df = tweets_df.loc[relevant_area_mask]\n",
    "    # The following mask accounts for both tweets with GPS coordinates and\n",
    "    # tweets within places which are a point.\n",
    "    has_gps = tweets_df['area'] == 0\n",
    "    # Here the tweets with coordinates outside the grid are out, because of the\n",
    "    # inner join\n",
    "    tweets_cells_df = geopd.sjoin(tweets_df.loc[has_gps], cells_in_area_df,\n",
    "        op='within', rsuffix='cell', how='inner')\n",
    "    nr_out_tweets =  len(tweets_df.loc[has_gps]) - len(tweets_cells_df)\n",
    "    print(f'{nr_out_tweets} tweets have been found outside of the grid and'\n",
    "         ' filtered out as a result.')\n",
    "    tweets_places_df = tweets_df.loc[~has_gps]\n",
    "    return tweets_cells_df, tweets_places_df\n",
    "    \n",
    "with mp.Pool(8) as pool:\n",
    "    map_parameters = [(res, max_place_area, cells_in_area_df) \n",
    "                      for res in tweets_process_res]\n",
    "    print('entering the loop')\n",
    "    tweets_pre_cell_res = (\n",
    "        pool.starmap_async(get_langs_counts, map_parameters).get())\n",
    "\n",
    "cells_langs_counts = None\n",
    "places_langs_counts = None\n",
    "\n",
    "for res in tweets_pre_cell_res:\n",
    "    tweets_cells_df = res[0]\n",
    "    tweets_places_df = res[1]\n",
    "    groupby_cols = ['cld_lang', 'cell_id']\n",
    "    cells_langs_counts = join_and_count.increment_counts(\n",
    "        cells_langs_counts, tweets_cells_df, groupby_cols)\n",
    "    groupby_cols = ['cld_lang', 'place_id']\n",
    "    places_langs_counts = join_and_count.increment_counts(\n",
    "        places_langs_counts, tweets_places_df, groupby_cols)\n",
    "\n",
    "places_langs_counts = places_langs_counts['count']\n",
    "places_counts = (places_langs_counts.groupby('place_id')\n",
    "                                   .sum()\n",
    "                                   .rename('total_count')\n",
    "                                   .to_frame())\n",
    "cells_langs_counts = cells_langs_counts['count']\n",
    "cells_counts = (cells_langs_counts.groupby('cell_id')\n",
    "                                  .sum()\n",
    "                                  .rename('total_count')\n",
    "                                  .to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Places -> cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We count the number of users speaking a local language in each cell and place \n",
    "# of residence.\n",
    "local_langs = [lang for lang in plot_langs_dict]\n",
    "places_local_counts = places_langs_counts.reset_index(level='cld_lang')\n",
    "local_langs_mask = places_local_counts['cld_lang'].isin(local_langs)\n",
    "places_local_counts = (places_local_counts.loc[local_langs_mask]\n",
    "                                          .groupby('place_id')['count']\n",
    "                                          .sum()\n",
    "                                          .rename('local_count'))\n",
    "places_counts = places_counts.join(places_local_counts, how='left')\n",
    "\n",
    "cells_local_counts = cells_langs_counts.reset_index(level='cld_lang')\n",
    "local_langs_mask = cells_local_counts['cld_lang'].isin(local_langs)\n",
    "cells_local_counts = (cells_local_counts.loc[local_langs_mask]\n",
    "                                        .groupby('cell_id')['count']\n",
    "                                        .sum()\n",
    "                                        .rename('local_count'))\n",
    "cells_counts = cells_counts.to_frame().join(cells_local_counts, how='left')\n",
    "\n",
    "cell_plot_df = places_to_cells.get_counts(\n",
    "    places_counts, places_langs_counts, places_geodf,\n",
    "    cells_in_area_df, plot_langs_dict, xy_proj=xy_proj)\n",
    "\n",
    "# We add the counts from the tweets with coordinates\n",
    "cell_plot_df = join_and_count.increment_join(\n",
    "    cell_plot_df, cells_counts['total_count'], count_col='total_count')\n",
    "cell_plot_df = join_and_count.increment_join(\n",
    "    cell_plot_df, cells_counts['local_count'], count_col='local_count')\n",
    "cell_plot_df = cell_plot_df.loc[cell_plot_df['total_count'] > 0]\n",
    "\n",
    "for plot_lang, lang_dict in plot_langs_dict.items():\n",
    "    lang_count_col = lang_dict['count_col']\n",
    "    cells_lang_counts = cells_langs_counts.xs(plot_lang).rename(lang_count_col)\n",
    "    cell_plot_df = join_and_count.increment_join(\n",
    "        cell_plot_df, cells_lang_counts, count_col=lang_count_col)\n",
    "    \n",
    "    level_lang_label = tweet_level_label.format(lang_dict['readable'])\n",
    "    sum_lang = cell_plot_df[lang_count_col].sum()\n",
    "    print(f'There are {sum_lang:.0f} {level_lang_label}.')\n",
    "    \n",
    "cell_plot_df['cell_id'] = cell_plot_df.index\n",
    "cell_data_path = cell_data_path_format.format('tweets', cc, cell_size)\n",
    "cell_plot_df.to_file(cell_data_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = 10000\n",
    "cell_data_path = cell_data_path_format.format('tweets', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(cell_plot_df, plot_langs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_lang, plot_dict in plot_langs_dict.items():\n",
    "    count_lang_col = plot_dict['count_col']\n",
    "    readable_lang = plot_dict['readable']\n",
    "    save_path = os.path.join(fig_dir, 'counts',\n",
    "        f'tweet_counts_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Distribution of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['count_label']\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='Purples')\n",
    "    ax_count = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=count_lang_col, save_path=save_path, \n",
    "        show=False, log_scale=True, title=plot_title, cbar_label=cbar_label,\n",
    "        xy_proj=xy_proj, **plot_kwargs)\n",
    "    \n",
    "    prop_lang_col = plot_dict['prop_col']\n",
    "    save_path = os.path.join(fig_dir, 'prop',\n",
    "        f'tweets_prop_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = '{} predominance in {}'.format(readable_lang, country_name)\n",
    "    cbar_label = plot_dict['prop_label']\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax_prop = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=prop_lang_col, save_path=save_path, \n",
    "        title=plot_title, cbar_label=cbar_label, vmin=0, vmax=1, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(fig_dir, \n",
    "            f'tweets_prop_cc={cc}_cell_size={cell_size}m.html')\n",
    "prop_dict = {'name': 'prop', 'readable': 'proportion', 'vmin': 0, 'vmax': 1}\n",
    "fig = grid_viz.plot_interactive(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, prop_dict,\n",
    "    save_path=save_path, plotly_renderer='iframe_connected', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study at the user level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users who have tagged their tweets with gps coordinates seem to do it regularly, as the median of the proportion of tweets they geo tag is at more than 75% on the first chunk -> it's worth it to try and get their cell of residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tweets_process_res[0].copy()\n",
    "a['has_gps'] = a['area'] == 0\n",
    "gps_uids = a.loc[a['has_gps'], 'uid'].unique()\n",
    "a = a.loc[a['uid'].isin(gps_uids)].groupby(['uid', 'has_gps']).size().rename('count').to_frame()\n",
    "a = a.join(a.groupby('uid')['count'].sum().rename('sum'))\n",
    "b = a.reset_index()\n",
    "b = b.loc[b['has_gps']]\n",
    "b['ratio'] = b['count'] / b['sum']\n",
    "b['ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's one or more cells where a user tweeted in proportion more than relevant_th of the time, we take among these cells the one where they tweeted the most outside work hours. Otherwise, we take the relevant place where they tweeted the most outside work hours, or we default to the place where they tweeted the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_label = '{}-speaking users'\n",
    "relevant_th = 0.1\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language(s) attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we get rid of users whose language we couldn't identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residence attribution is the longest to run, and by a long shot, so we'll start\n",
    "# with language to filter out uids in tweets_df before doing it\n",
    "groupby_cols = ['uid', 'cld_lang']\n",
    "user_langs_counts = None\n",
    "for res in tweets_process_res:\n",
    "    tweets_lang_df = res.copy()\n",
    "    # Here we don't filter out based on max_place_area, because these tweets\n",
    "    # are still useful for language attribution.\n",
    "    tweets_lang_df = tweets_lang_df.loc[tweets_lang_df['cld_lang'].notnull()]\n",
    "    user_langs_counts = join_and_count.increment_counts(\n",
    "        user_langs_counts, tweets_lang_df, groupby_cols)\n",
    "    \n",
    "total_per_user = user_langs_counts.groupby('uid')['count'].sum().rename('user_count')\n",
    "user_langs_agg = user_langs_counts.join(total_per_user).assign(\n",
    "    prop_lang=lambda df: df['count'] / df['user_count'])\n",
    "user_langs_agg = user_langs_agg.loc[user_langs_agg['prop_lang'] > relevant_th]\n",
    "uid_with_lang = user_langs_agg.index.levels[0].values\n",
    "print(f'We were able to attribute at least one language to {len(uid_with_lang)}'\n",
    "      ' users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_langs_counts = (user_langs_agg.groupby('cld_lang')\n",
    "                                   .size()\n",
    "                                   .rename('count')\n",
    "                                   .sort_values(ascending=False))\n",
    "total_count = len(user_langs_agg.index.levels[0])\n",
    "top_langs = area_langs_counts.index.values[:10]\n",
    "top_counts = area_langs_counts.values[:10]\n",
    "\n",
    "plt.bar(top_langs, top_counts)\n",
    "plt.title(f'Ten languages with the most speakers in {country_name}')\n",
    "plt.ylabel('number of speakers')\n",
    "save_path = os.path.join(fig_dir, f'top_langs_speakers_count_cc={cc}.pdf')\n",
    "plt.savefig(save_path)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.bar(top_langs, top_counts/total_count)\n",
    "plt.title(f'Ten languages with the most speakers in {country_name}')\n",
    "save_path = os.path.join(fig_dir, f'top_langs_speakers_prop_cc={cc}.pdf')\n",
    "plt.ylabel('proportion of the users speaking')\n",
    "plt.savefig(save_path)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute users to a group: mono, bi, tri, ... lingual\n",
    "\n",
    "Problem: need more tweets to detect multilingualism, eg users with only three tweets in the dataset are very unlikely to be detected as multilinguals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_langs = [lang for lang in plot_langs_dict]\n",
    "users_ling_grp = user_langs_agg.reset_index(level='cld_lang')\n",
    "# user_langs_agg is sorted by user and language, because of the groupby in\n",
    "# increment counts. Thus, when we concatenate the languages with sum() here,\n",
    "# they're already sorted so we won't get both 'frit' and 'itfr' for instance.\n",
    "local_mask = users_ling_grp['cld_lang'].isin(local_langs)\n",
    "users_ling_grp = (users_ling_grp.loc[local_mask, 'cld_lang']\n",
    "                                .groupby('uid')\n",
    "                                .apply(lambda langs: 'ling_'+langs.sum())\n",
    "                                .rename('ling_grp')\n",
    "                                .to_frame()\n",
    "                                .groupby(['uid', 'ling_grp'])\n",
    "                                .first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_counts = (users_ling_grp.reset_index()\n",
    "                            .groupby('ling_grp')\n",
    "                            .size()\n",
    "                            .sort_values(ascending=False))\n",
    "multiling_grps = ling_counts.index.values\n",
    "x_plot = [grp[5:] for grp in multiling_grps]\n",
    "plt.bar(x_plot, ling_counts)\n",
    "plt.title(f'Local languages groups in {country_name}')\n",
    "plt.ylabel('number of speakers')\n",
    "save_path = os.path.join(fig_dir, f'multilinguals_count_cc={cc}.pdf')\n",
    "plt.savefig(save_path)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.bar(x_plot, ling_counts/total_count)\n",
    "plt.title(f'Local languages groups in {country_name}')\n",
    "save_path = os.path.join(fig_dir, f'multilinguals_prop_cc={cc}.pdf')\n",
    "plt.ylabel('proportion of the users speaking')\n",
    "plt.savefig(save_path)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-residence attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_resid_attr(tweets_lang_df, cells_in_area_df, uid_with_lang, \n",
    "                    max_place_area, cc_timezone):\n",
    "    tweets_df = tweets_lang_df.copy()\n",
    "    # We filter out users to which we couldn't attribute even one language\n",
    "    uid_mask = tweets_df['uid'].isin(uid_with_lang)\n",
    "    relevant_area_mask = tweets_df['area'] < max_place_area\n",
    "    tweets_df = tweets_df.loc[uid_mask & relevant_area_mask].copy()\n",
    "    tweets_df['hour'] = (tweets_df['created_at'].dt.tz_localize('UTC')\n",
    "                                                .dt.tz_convert(cc_timezone)\n",
    "                                                .dt.hour)\n",
    "    # Tweets are considered in work hours if they were made between 8 and 18\n",
    "    # outside of the week-end (weekday goes from 0 (Monday) to 6 (Sunday)).\n",
    "    tweets_df['isin_workhour'] = (\n",
    "        (tweets_df['hour'] > 7) \n",
    "        & (tweets_df['hour'] < 18)\n",
    "        & (tweets_df['created_at'].dt.weekday < 5))\n",
    "    \n",
    "    has_gps = tweets_df['area'] == 0\n",
    "    tweets_cells_df = geopd.sjoin(tweets_df.loc[has_gps], cells_in_area_df, \n",
    "        op='within', rsuffix='cell', how='inner')\n",
    "    # geopd adds an underscore by itself to the suffix\n",
    "    tweets_places_df = tweets_df.loc[~has_gps]\n",
    "    print('chunk done')\n",
    "    return tweets_cells_df, tweets_places_df\n",
    "\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    map_parameters = [(res, cells_in_area_df, uid_with_lang, max_place_area, \n",
    "                       cc_timezone) \n",
    "                      for res in tweets_process_res]\n",
    "    print('entering the loop')\n",
    "    tweets_pre_resid_res = (\n",
    "        pool.starmap_async(prep_resid_attr, map_parameters).get())\n",
    "    \n",
    "user_places_habits = None\n",
    "user_cells_habits = None\n",
    "for res in tweets_pre_resid_res:\n",
    "    # We first count the number of times a user has tweeted in each place inside\n",
    "    # and outside work hours.\n",
    "    tweets_places_df = res[1]\n",
    "    groupby_cols = ['uid', 'place_id', 'isin_workhour']\n",
    "    user_places_habits = join_and_count.increment_counts(\n",
    "        user_places_habits, tweets_places_df, groupby_cols)\n",
    "    # Then we do the same thing except in each cell, using the tweets with\n",
    "    # coordinates.\n",
    "    tweets_cells_df = res[0]\n",
    "    groupby_cols = ['uid', 'cell_id', 'isin_workhour']\n",
    "    user_cells_habits = join_and_count.increment_counts(\n",
    "        user_cells_habits, tweets_cells_df, groupby_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we took number of speakers, whether they're multilingual or monolingual, if they speak a language, they count as one in that language's count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possibility: pass the places counts to cells counts here, and then do the whole residence attribution solely\n",
    "on a cell basis. Problem: user tags himself in the same city all the time, overlapping multiple cells: we'll have more than one cell with approximately the same count, and one cells takes all in the end. Typical 'winner takes all' problem in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residence attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the total number of users in each cell and place of residence.\n",
    "user_counts_in_cells = user_cells_habits.groupby('uid')['count'].sum().rename('user_count')\n",
    "user_home_cell = user_cells_habits.join(user_counts_in_cells, how='inner')\n",
    "user_home_cell['prop_in_cell'] = user_home_cell['count'] / user_home_cell['user_count']\n",
    "user_home_cell = (user_home_cell.loc[user_home_cell['prop_in_cell'] > relevant_th]\n",
    "                                .xs(False, level='isin_workhour')\n",
    "                                .reset_index()\n",
    "                                .sort_values(by=['uid', 'count'])\n",
    "                                .groupby('uid')['cell_id']\n",
    "                                .last())\n",
    "\n",
    "users_with_cell = user_home_cell.index.values\n",
    "user_home_place = uagg.get_residence(user_places_habits, place_id_col='place_id')\n",
    "user_only_place = user_home_place.reset_index()\n",
    "user_only_place = (\n",
    "    user_only_place.loc[~user_only_place['uid'].isin(users_with_cell)]\n",
    "                   .set_index('uid')\n",
    "                   .loc[:, 'place_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate cell data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: remake plot_langs_dict to have labels for mono, bi, tri linguals, total and local and col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get all the places with residents and the associated count\n",
    "places_counts = (user_only_place.to_frame()\n",
    "                                .groupby('place_id')\n",
    "                                .size()\n",
    "                                .rename('total_count')\n",
    "                                .to_frame())\n",
    "cells_counts = (user_home_cell.to_frame()\n",
    "                              .groupby('cell_id')\n",
    "                              .size()\n",
    "                              .rename('total_count')\n",
    "                              .to_frame())\n",
    "# We count the number of users speaking a local language in each cell and place \n",
    "# of residence.\n",
    "local_lang_users = user_langs_agg.reset_index(level='cld_lang')\n",
    "local_langs = [lang for lang in plot_langs_dict]\n",
    "local_langs_mask = local_lang_users['cld_lang'].isin(local_langs)\n",
    "local_lang_users = (local_lang_users.loc[local_langs_mask]\n",
    "                                    .groupby('uid')\n",
    "                                    .first())\n",
    "places_local_counts = uagg.to_count_by_area(local_lang_users, user_only_place,\n",
    "                                            output_col='local_count')\n",
    "cells_local_counts = uagg.to_count_by_area(local_lang_users, user_home_cell, \n",
    "                                           output_col='local_count')\n",
    "# Then we get the counts of speakers by language and cell\n",
    "places_langs_counts = uagg.to_count_by_area(user_langs_agg, user_only_place)\n",
    "cells_langs_counts = uagg.to_count_by_area(user_langs_agg, user_home_cell)\n",
    "# Then the counts of groups (mono-, bi-, tri-linguals):\n",
    "places_ling_counts = uagg.to_count_by_area(users_ling_grp, user_only_place)\n",
    "cells_ling_counts = uagg.to_count_by_area(users_ling_grp, user_home_cell)\n",
    "\n",
    "# We always left join on places counts, because total_count == 0 implies\n",
    "# that every other count is 0.\n",
    "places_counts = places_counts.join(places_local_counts, how='left')\n",
    "cells_counts = cells_counts.join(cells_local_counts, how='left')\n",
    "# TODO:TODO regroup in one for loop\n",
    "for ling in multiling_grps:\n",
    "    ling_count_col= f'count_{ling}'\n",
    "    cells_in_that_grp_count = (cells_ling_counts.xs(ling, level='ling_grp')\n",
    "                                                .rename(ling_count_col))\n",
    "    places_in_that_grp_count = (places_ling_counts.xs(ling, level='ling_grp')\n",
    "                                                  .rename(ling_count_col))\n",
    "    cells_counts = cells_counts.join(cells_in_that_grp_count, how='left')\n",
    "    places_counts = places_counts.join(places_in_that_grp_count, how='left')\n",
    "    \n",
    "for plot_lang, lang_dict in plot_langs_dict.items():\n",
    "    lang_count_col = lang_dict['count_col']\n",
    "    places_lang_counts = (places_langs_counts.xs(plot_lang, level='cld_lang')\n",
    "                                             .rename(lang_count_col))\n",
    "    cells_lang_counts = (cells_langs_counts.xs(plot_lang, level='cld_lang')\n",
    "                                           .rename(lang_count_col))\n",
    "    places_counts = places_counts.join(places_lang_counts, how='left')\n",
    "    cells_counts = cells_counts.join(cells_lang_counts, how='left')\n",
    "\n",
    "cell_plot_df = cells_in_area_df.copy()\n",
    "cells_in_places = places_to_cells.get_intersect(cell_plot_df, places_geodf, \n",
    "                                                places_counts, xy_proj=xy_proj)\n",
    "\n",
    "# Then we add the total counts for the users with a cell of residence.\n",
    "count_cols = places_counts.columns\n",
    "cell_plot_df = places_to_cells.intersect_to_cells(\n",
    "    cells_in_places, cell_plot_df, count_cols)\n",
    "for col in count_cols:\n",
    "    cell_plot_df = join_and_count.increment_join(\n",
    "        cell_plot_df, cells_counts[col], count_col=col)\n",
    "# 0 or 1?\n",
    "cell_plot_df = cell_plot_df.loc[cell_plot_df['total_count'] > 0]\n",
    "\n",
    "for plot_lang, lang_dict in plot_langs_dict.items():\n",
    "    lang_count_col = lang_dict['count_col']\n",
    "    level_lang_label = user_level_label.format(lang_dict['readable'])\n",
    "    sum_lang = cell_plot_df[lang_count_col].sum()\n",
    "    print(f'There are {sum_lang:.0f} {level_lang_label}.')\n",
    "\n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df.to_file(cell_data_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: save for every cc and cell_size with correct cell_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_size = 10000\n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(cell_plot_df, \n",
    "                                                     plot_langs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_lang, plot_dict in plot_langs_dict.items():\n",
    "    count_lang_col = plot_dict['count_col']\n",
    "    readable_lang = plot_dict['readable']\n",
    "    save_path = os.path.join(fig_dir, 'counts',\n",
    "        f'users_counts_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Distribution of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['count_label']\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='Purples')\n",
    "    ax_count = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=count_lang_col, save_path=save_path, \n",
    "        show=False, log_scale=True, title=plot_title, cbar_label=cbar_label,\n",
    "        xy_proj=xy_proj, **plot_kwargs)\n",
    "    \n",
    "    prop_lang_col = plot_dict['prop_col']\n",
    "    save_path = os.path.join(fig_dir, 'prop',\n",
    "        f'users_prop_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Predominance of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['prop_label']\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax_prop = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=prop_lang_col, save_path=save_path, \n",
    "        title=plot_title, cbar_label=cbar_label, vmin=0, vmax=1, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(fig_dir, \n",
    "            f'users_prop_cc={cc}_cell_size={cell_size}m.html')\n",
    "prop_dict = {'name': 'prop', 'readable': 'proportion', 'vmin': 0, 'vmax': 1}\n",
    "fig = grid_viz.plot_interactive(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, prop_dict,\n",
    "    save_path=save_path, plotly_renderer='iframe_connected', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "272px",
    "width": "238px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 705.85,
   "position": {
    "height": "727.85px",
    "left": "256px",
    "right": "20px",
    "top": "121px",
    "width": "723px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
