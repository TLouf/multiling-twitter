{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all src modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cProfile\n",
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    import cld3\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "import pycld2\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import src.utils.geometry as geo\n",
    "import src.utils.places_to_cells as places_to_cells\n",
    "import src.data.shp_extract as shp_extract\n",
    "import src.data.tweets_cells_counts as tweets_counts\n",
    "import src.data.text_process as text_process\n",
    "import src.data.access as data_access\n",
    "import src.visualization.grid_viz as grid_viz\n",
    "import src.data.user_filters as ufilters\n",
    "import src.data.user_agg as uagg\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container </style>\"))\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = os.environ['DATA_DIR']\n",
    "tweets_files_format = 'tweets_2015_2018_{}.json.gz'\n",
    "places_files_format = 'places_2015_2018_{}.json.gz'\n",
    "ssh_domain = os.environ['IFISC_DOMAIN']\n",
    "ssh_username = os.environ['IFISC_USERNAME']\n",
    "project_data_dir = os.path.join('..', 'data')\n",
    "external_data_dir = os.path.join(project_data_dir, 'external')\n",
    "interim_data_dir = os.path.join(project_data_dir, 'interim')\n",
    "processed_data_dir = os.path.join(project_data_dir, 'processed')\n",
    "cell_data_path_format = os.path.join(processed_data_dir,\n",
    "                                     '{}_cell_data_cc={}_cell_size={}m.geojson')\n",
    "latlon_proj = 'epsg:4326'\n",
    "LANGS_DICT = dict([(lang[1],lang[0].lower().capitalize())\n",
    "                   for lang in pycld2.LANGUAGES])\n",
    "\n",
    "country_codes = ('BO', 'CA', 'CH', 'EE', 'ES', 'FR', 'HK', 'ID', 'LT', 'LV',\n",
    "                 'MY', 'PE', 'RO', 'SG', 'TN', 'UA')\n",
    "with open(os.path.join(external_data_dir, 'countries.json')) as f:\n",
    "    countries_study_data = json.load(f)\n",
    "\n",
    "# Country specific parameters\n",
    "cc = 'CH'\n",
    "fig_dir = os.path.join('..', 'reports', 'figures', cc)\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.mkdir(os.path.join(fig_dir, 'counts'))\n",
    "    os.mkdir(os.path.join(fig_dir, 'prop'))\n",
    "xy_proj = countries_study_data[cc]['xy_proj']\n",
    "plot_langs_list = countries_study_data[cc]['local_langs']\n",
    "valid_uids_path = os.path.join(interim_data_dir, f'valid_uids_{cc}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of bots, company account (eg careerarc, tweetmyjobs). If can't distinguish companies by source like careerarc, then how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places, area and grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_name = 'CNTR_RG_01M_2016_4326.shp'\n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_name, shapefile_name)\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = shape_df.loc[shape_df['FID'] == cc]\n",
    "country_name = shape_df['NAME_ENGL'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Places can be a point too -> treat them like tweets with coords in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_from_bbox(bbox):\n",
    "    bbox = bbox['coordinates'][0]\n",
    "    geo = Polygon(bbox)\n",
    "    area = geo.area\n",
    "    if area == 0:\n",
    "        geo = Point(bbox[0])\n",
    "    return geo, area\n",
    "\n",
    "places_file_path = os.path.join(data_dir_path, places_files_format.format(cc))\n",
    "raw_places_df = data_access.return_json(places_file_path,\n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, compression='gzip')\n",
    "places_df = raw_places_df[['id', 'bounding_box', 'name', 'place_type']].copy()\n",
    "places_df['geometry'], places_df['area'] = zip(\n",
    "    *places_df['bounding_box'].apply(geo_from_bbox))\n",
    "places_geodf = geopd.GeoDataFrame(\n",
    "    places_df, crs=latlon_proj, geometry=places_df['geometry'])\n",
    "places_geodf['area'] = area\n",
    "places_geodf = places_geodf.set_index('id', drop=False)\n",
    "# Since the places' bbox can stretch outside of the whole shape, we need to \n",
    "# take the intersection between the two. However, we only use the overlay to \n",
    "# calculate the area, so that places_to_cells distributes the whole population \n",
    "# of a place to the different cells within it. However we don't need the actual \n",
    "# geometry from the intersection, which is more complex and thus slows down \n",
    "# computations later on.\n",
    "poly_mask = places_geodf['area'] > 0\n",
    "polygons_in_shape = geopd.overlay(\n",
    "    shape_df[['geometry']], places_geodf.loc[poly_mask], how='intersection')\n",
    "polygons_in_shape = polygons_in_shape.set_index('id')\n",
    "places_geodf.loc[poly_mask, 'area'] = polygons_in_shape.to_crs(xy_proj).area\n",
    "places_geodf = places_geodf.drop(columns=['bounding_box', 'id'])\n",
    "places_in_xy = places_geodf.geometry.to_crs(xy_proj)\n",
    "places_geodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = 5000\n",
    "max_place_area = 1e9 # linked to cell size and places data\n",
    "cells_df, cells_in_area_df = geo.create_grid(shape_df, cell_size, latlon_proj, \n",
    "                                             xy_proj, intersect=True)\n",
    "grid_test_df = cells_in_area_df.copy()\n",
    "grid_test_df['metric'] = 1\n",
    "save_path = os.path.join(fig_dir, f'grid_cc={cc}_cell_size={cell_size}m.pdf')\n",
    "plot_kwargs = dict(alpha=0.7, edgecolor='w', linewidths=0.5, cmap='Purples')\n",
    "ax = grid_viz.plot_grid(grid_test_df, shape_df, metric_col='metric', show=True, \n",
    "                        save_path=save_path, xy_proj=xy_proj, **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_path = os.path.join(data_dir_path, tweets_files_format.format(cc))\n",
    "\n",
    "def read_data(tweets_file_path, chunk_start, chunk_size):\n",
    "    raw_tweets_df = data_access.read_json_wrapper(\n",
    "        tweets_file_path, chunk_start, chunk_size, ssh_domain=ssh_domain,\n",
    "        ssh_username=ssh_username)\n",
    "    return raw_tweets_df\n",
    "\n",
    "def profile_pre_process(tweets_file_path, chunk_start, chunk_size):\n",
    "    cProfile.runctx('read_data(tweets_file_path, chunk_start, chunk_size)', \n",
    "                    globals(), locals())\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    tweets_access_res = []\n",
    "    for chunk_start, chunk_size in data_access.chunkify(\n",
    "            tweets_file_path, size=1e9, ssh_domain=ssh_domain, \n",
    "            ssh_username=ssh_username):\n",
    "        tweets_access_res.append(pool.apply_async(\n",
    "            read_data, (tweets_file_path, chunk_start, chunk_size)))\n",
    "    \n",
    "    # This is mandatory so that the pool doesn't stop working until every\n",
    "    # chunk has been processed.\n",
    "    for res in tweets_access_res:\n",
    "        res.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeted_months = None\n",
    "tweets_pb_months = None\n",
    "for res in tweets_access_res:\n",
    "    tweets_df = res.get().copy()\n",
    "    tweets_df['month'] = tweets_df['created_at'].dt.to_period('M')\n",
    "    has_gps = tweets_df['coordinates'].notnull()\n",
    "    geometry = tweets_df.loc[has_gps, 'coordinates'].apply(lambda x: Point(x['coordinates']))\n",
    "    tweets_coords = geopd.GeoSeries(geometry, crs=latlon_proj, index=tweets_df.loc[has_gps].index)\n",
    "    tweets_df = tweets_df.join(places_geodf, on='place_id', how='left')\n",
    "    coords_in_place = tweets_coords.within(geopd.GeoSeries(tweets_df.loc[has_gps, 'geometry']))\n",
    "    \n",
    "    tweeted_months = tweets_counts.increment_counts(\n",
    "        tweeted_months, tweets_df, ['month'])\n",
    "    tweets_pb_months = tweets_counts.increment_counts(\n",
    "        tweets_pb_months, tweets_df.loc[has_gps].loc[~coords_in_place], ['month'])\n",
    "#     print(coords_in_place.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_counts = tweeted_months.join(tweets_pb_months, rsuffix='_pb', how='left')\n",
    "months_counts['prop'] = months_counts['count_pb'] / months_counts['count']\n",
    "ax = months_counts['prop'].plot.bar()\n",
    "ticks = np.arange(0,47,5)\n",
    "tick_labels = ax.get_xticklabels()\n",
    "_ = ax.set_xticks(ticks)\n",
    "_ = ax.set_xticklabels([tick_labels[i] for i in ticks])\n",
    "_ = ax.set_ylabel('proportion')\n",
    "_ = ax.set_title('Proportion of tweets with coords outside of place')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters: user-based imply a loop over all the raw_tweets_df, and must be applied before getting tweets_lang_df and even tweets_loc_df, because these don't interest us at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is incremental, so can't parallelize. And it's rather fast, so not worth the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeted_months_users = None\n",
    "for res in tweets_access_res:\n",
    "    raw_tweets_df = res.get()\n",
    "    nr_users = len(raw_tweets_df['uid'].unique())\n",
    "    print(f'There are {nr_users} distinct users in this chunk.')\n",
    "    tweeted_months_users = ufilters.inc_months_activity(\n",
    "        tweeted_months_users, raw_tweets_df)\n",
    "\n",
    "tweeted_months_users = tweeted_months_users['count']\n",
    "total_nr_users = len(tweeted_months_users.index.levels[0])\n",
    "print(f'In total, there are {total_nr_users} distinct users in the whole dataset.')\n",
    "local_uids = ufilters.consec_months(tweeted_months_users)\n",
    "bot_uids = ufilters.bot_activity(tweeted_months_users)\n",
    "# We have local_uids: index of uids with a column full of True, and bot_uids:\n",
    "# index of uids with a column full of False. When we multiply them, the uids\n",
    "# in local_uids which are not in bot_uids are assigned NaN, and the ones which \n",
    "# are in bot_uids are assigned False. When we convert to the boolean type,\n",
    "# the NaNs turn to True.\n",
    "valid_uids = (local_uids * bot_uids).astype('bool').rename('valid')\n",
    "valid_uids = valid_uids.loc[valid_uids]\n",
    "print(f'This leaves us with {len(valid_uids)} valid users in the whole dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_filter(raw_tweets_df, valid_uids, places_in_xy, max_distance):\n",
    "    tweets_df = raw_tweets_df.join(valid_uids, on='uid', how='inner')\n",
    "    too_fast_uids = ufilters.too_fast(tweets_df, places_in_xy, max_distance)\n",
    "    return too_fast_uids\n",
    "\n",
    "too_fast_uids_series = pd.Series([])\n",
    "area_bounds = shape_df.to_crs(xy_proj).geometry.iloc[0].bounds\n",
    "# Get an upper limit of the distance that can be travelled inside the area\n",
    "max_distance = np.sqrt((area_bounds[0]-area_bounds[2])**2 \n",
    "                       + (area_bounds[1]-area_bounds[3])**2)\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    cols = ['uid', 'created_at', 'place_id', 'coordinates']\n",
    "    map_parameters = [\n",
    "        (res.get().loc[:, cols], valid_uids, places_in_xy, max_distance) \n",
    "        for res in tweets_access_res]\n",
    "    print('entering the loop')\n",
    "    too_fast_uids_list = pool.starmap_async(speed_filter, map_parameters).get()\n",
    "    for too_fast_uids in too_fast_uids_list:\n",
    "        too_fast_uids_series = (too_fast_uids_series * too_fast_uids).fillna(False)\n",
    "\n",
    "print(f'In total, there are {len(too_fast_uids_series)} too fast users left to '\n",
    "      'filter out in the whole dataset.')\n",
    "valid_uids = (valid_uids * too_fast_uids_series).astype('bool').rename('valid')\n",
    "valid_uids = valid_uids.loc[valid_uids]\n",
    "print(f'This leaves us with {len(valid_uids)} valid users in the whole dataset.')\n",
    "valid_uids.index = valid_uids.index.rename('uid')\n",
    "valid_uids.to_csv(valid_uids_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most tweets in the month in that country to asign local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't filter out tweets with a useless place (one too large) here, because these tweets can still be useful for language detection. So this filter is only applied later on. Similarly, we keep tweets with insufficient text to make a reliable language detection, because they can still be useful for residence attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_uids = pd.read_csv(valid_uids_path, index_col='uid', header=0)\n",
    "\n",
    "def process(raw_tweets_df, valid_uids, places_geodf, text_col='text', \n",
    "            min_nr_words=4, cld='pycld2'):\n",
    "    cols = ['text', 'id', 'lang', 'place_id', 'coordinates', 'uid', \n",
    "            'created_at', 'source']\n",
    "    tweets_loc_df = raw_tweets_df.loc[:, cols]\n",
    "    print('- starting geo join')\n",
    "    tweets_loc_df = tweets_loc_df.join(valid_uids, on='uid', how='inner')\n",
    "    has_gps = tweets_loc_df['coordinates'].notnull()\n",
    "    tweets_places_df = tweets_loc_df.loc[~has_gps].join(\n",
    "        places_geodf[['geometry', 'area']], on='place_id', how='left')\n",
    "    # The geometry of the tweets with GPS coordinates is the Point associated \n",
    "    # to them.\n",
    "    tweets_loc_df.loc[has_gps, 'geometry'] = tweets_loc_df.loc[has_gps, 'coordinates'].apply(\n",
    "        lambda x: Point(x['coordinates']))\n",
    "    # We assign the area of points to 0, and at the same time initialize the \n",
    "    # whole column, whose values will change for tweets without GPS coordinates.\n",
    "    tweets_loc_df['area'] = 0\n",
    "    # We add the geometry of the place to the tweets without GPS coordinates\n",
    "    tweets_loc_df.loc[~has_gps, 'geometry'] = tweets_places_df['geometry']\n",
    "    tweets_loc_df.loc[~has_gps, 'area'] = tweets_places_df['area']\n",
    "    tweets_loc_df = (tweets_loc_df.rename(columns={'lang': 'twitter_lang'})\n",
    "                                  .drop(columns=['valid', 'coordinates']))\n",
    "    tweets_loc_df = geopd.GeoDataFrame(tweets_loc_df, crs=latlon_proj)\n",
    "    print('starting lang detect')\n",
    "    tweets_lang_df = text_process.lang_detect(\n",
    "        tweets_loc_df, text_col='text', min_nr_words=4, cld='pycld2')\n",
    "    print('chunk done')\n",
    "    return tweets_lang_df\n",
    "\n",
    "\n",
    "def profile_process(raw_tweets_df, valid_uids, places_geodf):\n",
    "    cProfile.runctx(\n",
    "        'process(raw_tweets_df, valid_uids, places_geodf)', globals(), locals())\n",
    "\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    map_parameters = [(res.get(), valid_uids, places_geodf) \n",
    "                      for res in tweets_access_res]\n",
    "    print('entering the loop')\n",
    "    tweets_process_res = pool.starmap_async(process, map_parameters).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study at the tweet level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tweet counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_level_label = 'tweets in {}'\n",
    "\n",
    "plot_langs_dict = {}\n",
    "for plot_lang in plot_langs_list:\n",
    "    readable_lang = LANGS_DICT[plot_lang]\n",
    "    lang_count_col = f'cell_count_{plot_lang}'\n",
    "    lang_prop_col = f'cell_prop_{plot_lang}'\n",
    "    level_lang_label = tweet_level_label.format(readable_lang)\n",
    "    lang_count_label = f'Number of {level_lang_label} in the cell'\n",
    "    lang_prop_label = f'Proportion of {level_lang_label} in the cell'\n",
    "    lang_dict = {'prop_col': lang_prop_col,\n",
    "                 'count_col': lang_count_col,\n",
    "                 'count_label': lang_count_label,\n",
    "                 'prop_label': lang_prop_label,\n",
    "                 'readable': readable_lang}\n",
    "    plot_langs_dict[plot_lang] = lang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why sjoin so slow? It tests on every cell, even though it's exclusive: if one cell matches no other will. Solution: loop over cells, ordered by the counts obtained from places, and stop at first match, will greatly reduce the number of 'within' operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_langs_counts = None\n",
    "cells_langs_counts = None\n",
    "for res in tweets_process_res:\n",
    "    tweets_df = res.copy()\n",
    "    relevant_area_mask = tweets_df['area'] < max_place_area\n",
    "    tweets_df = tweets_df.loc[relevant_area_mask]\n",
    "    has_gps = tweets_df['area'] == 0\n",
    "    # Here the tweets with coordinates outside the grid are out, because of the\n",
    "    # inner join\n",
    "    tweets_cells_df = geopd.sjoin(tweets_df.loc[has_gps], cells_in_area_df,\n",
    "        op='within', rsuffix='cell', how='inner')\n",
    "    nr_out_tweets =  len(tweets_df.loc[has_gps]) - len(tweets_cells_df)\n",
    "    print(f'{nr_out_tweets} tweets have been found outside of the grid and'\n",
    "         ' filtered out as a result.')\n",
    "    # geopd adds an underscore by itself to the suffix\n",
    "    tweets_cells_df = tweets_cells_df.rename(columns={'index_cell': 'cell_id'})\n",
    "    groupby_cols = ['cld_lang', 'cell_id']\n",
    "    cells_langs_counts = tweets_counts.increment_counts(\n",
    "        cells_langs_counts, tweets_cells_df, groupby_cols)\n",
    "    tweets_places_df = tweets_df.loc[~has_gps]\n",
    "    groupby_cols = ['cld_lang', 'place_id']\n",
    "    places_langs_counts = tweets_counts.increment_counts(\n",
    "        places_langs_counts, tweets_places_df, groupby_cols)\n",
    "\n",
    "places_counts = places_langs_counts.groupby('place_id').sum()\n",
    "cells_langs_counts = cells_langs_counts['count']\n",
    "cells_counts = cells_langs_counts.groupby('cell_id').sum().rename('total_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Places -> cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_plot_df = places_to_cells.get_counts(\n",
    "    places_counts, places_langs_counts, places_geodf,\n",
    "    cells_in_area_df, plot_langs_dict, xy_proj=xy_proj)\n",
    "\n",
    "# We add the counts from the tweets with coordinates\n",
    "cell_plot_df = tweets_counts.increment_join(\n",
    "    cell_plot_df, cells_counts, count_col='total_count')\n",
    "cell_plot_df = cell_plot_df.loc[cell_plot_df['total_count'] > 0]\n",
    "\n",
    "for plot_lang, lang_dict in plot_langs_dict.items():\n",
    "    lang_count_col = lang_dict['count_col']\n",
    "    cells_lang_counts = cells_langs_counts.xs(plot_lang).rename(lang_count_col)\n",
    "    cell_plot_df = tweets_counts.increment_join(\n",
    "        cell_plot_df, cells_lang_counts, count_col=lang_count_col)\n",
    "    \n",
    "    level_lang_label = tweet_level_label.format(lang_dict['readable'])\n",
    "    sum_lang = cell_plot_df[lang_count_col].sum()\n",
    "    print(f'There are {sum_lang:.0f} {level_lang_label}.')\n",
    "    \n",
    "    lang_prop_col = lang_dict['prop_col']\n",
    "    cell_plot_df[lang_prop_col] = (cell_plot_df[lang_count_col]\n",
    "                                   / cell_plot_df['total_count'])\n",
    "\n",
    "cell_plot_df['cell_id'] = cell_plot_df.index\n",
    "cell_data_path = cell_data_path_format.format('tweets', cc, cell_size)\n",
    "cell_plot_df.to_file(cell_data_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_size = 10000\n",
    "cell_data_path = cell_data_path_format.format('tweets', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_lang, plot_dict in plot_langs_dict.items():\n",
    "    count_lang_col = plot_dict['count_col']\n",
    "    readable_lang = plot_dict['readable']\n",
    "    save_path = os.path.join(fig_dir, 'counts',\n",
    "        f'tweet_counts_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Distribution of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['count_label']\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='Purples')\n",
    "    ax_count = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=count_lang_col, save_path=save_path, \n",
    "        show=False, log_scale=True, title=plot_title, cbar_label=cbar_label,\n",
    "        xy_proj=xy_proj, **plot_kwargs)\n",
    "    \n",
    "    prop_lang_col = plot_dict['prop_col']\n",
    "    save_path = os.path.join(fig_dir, 'prop',\n",
    "        f'tweets_prop_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = '{} predominance in {}'.format(readable_lang, country_name)\n",
    "    cbar_label = plot_dict['prop_label']\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax_prop = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=prop_lang_col, save_path=save_path, \n",
    "        title=plot_title, cbar_label=cbar_label, vmax=1, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(fig_dir, \n",
    "            f'tweets_prop_cc={cc}_cell_size={cell_size}m.html')\n",
    "\n",
    "fig = grid_viz.plot_interactive(cell_plot_df, shape_df, plot_langs_dict,\n",
    "    save_path=save_path, plotly_renderer='iframe_connected', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study at the user level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make user counts data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users who have tagged their tweets with gps coordinates seem to do it regularly, as the median of the proportion of tweets they geo tag is at more than 75% on the first chunk -> it's worth it to try and get their cell of residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tweets_process_res[0].copy()\n",
    "a['has_gps'] = a['area'] == 0\n",
    "gps_uids = a.loc[a['has_gps'], 'uid'].unique()\n",
    "a = a.loc[a['uid'].isin(gps_uids)].groupby(['uid', 'has_gps']).size().rename('count').to_frame()\n",
    "a = a.join(a.groupby('uid')['count'].sum().rename('sum'))\n",
    "b = a.reset_index()\n",
    "b = b.loc[b['has_gps']]\n",
    "b['ratio'] = b['count'] / b['sum']\n",
    "b['ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's one or more cells where a user tweeted in proportion more than relevant_th of the time, we take among these cells the one where they tweeted the most outside work hours. Otherwise, we take the relevant place where they tweeted the most outside work hours, or we default to the place where they tweeted the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_label = '{}-speaking users'\n",
    "relevant_th = 0.1\n",
    "\n",
    "plot_langs_dict = {}\n",
    "for plot_lang in plot_langs_list:\n",
    "    readable_lang = LANGS_DICT[plot_lang]\n",
    "    lang_count_col = f'cell_count_{plot_lang}'\n",
    "    lang_prop_col = f'cell_prop_{plot_lang}'\n",
    "    level_lang_label = user_level_label.format(readable_lang)\n",
    "    lang_count_label = f'Number of {level_lang_label} in the cell'\n",
    "    lang_prop_label = f'Proportion of {level_lang_label} in the cell'\n",
    "    lang_dict = {'prop_col': lang_prop_col,\n",
    "                 'count_col': lang_count_col,\n",
    "                 'count_label': lang_count_label,\n",
    "                 'prop_label': lang_prop_label,\n",
    "                 'readable': readable_lang}\n",
    "    plot_langs_dict[plot_lang] = lang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get rid of users whose language we couldn't identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residence attribution is the longest to run, and by a long shot, so we'll start\n",
    "# with language to filter out uids in tweets_df before doing it\n",
    "groupby_cols = ['uid', 'cld_lang']\n",
    "user_langs_counts = None\n",
    "for res in tweets_process_res:\n",
    "    tweets_lang_df = res.copy()\n",
    "    # Here we don't filter out based on max_place_area, because these tweets\n",
    "    # are still useful for language attribution.\n",
    "    tweets_lang_df = tweets_lang_df.loc[tweets_lang_df['cld_lang'].notnull()]\n",
    "    user_langs_counts = tweets_counts.increment_counts(\n",
    "        user_langs_counts, tweets_lang_df, groupby_cols)\n",
    "    \n",
    "total_per_user = user_langs_counts.groupby('uid')['count'].sum().rename('user_count')\n",
    "user_langs_agg = user_langs_counts.join(total_per_user).assign(\n",
    "    prop_lang=lambda df: df['count'] / df['user_count'])\n",
    "user_langs_agg = user_langs_agg.loc[user_langs_agg['prop_lang'] > relevant_th]\n",
    "uid_with_lang = user_langs_agg.index.levels[0].values\n",
    "print(f'We were able to attribute at least one language to {len(uid_with_lang)}'\n",
    "      ' users')\n",
    "\n",
    "user_places_habits = None\n",
    "user_cells_habits = None\n",
    "for res in tweets_process_res:\n",
    "    tweets_df = res.copy()\n",
    "    # We filter out users to which we couldn't attribute even one language\n",
    "    uid_mask = tweets_df['uid'].isin(uid_with_lang)\n",
    "    relevant_area_mask = tweets_df['area'] < max_place_area\n",
    "    tweets_df = tweets_df.loc[uid_mask & relevant_area_mask].copy()\n",
    "    tweets_df['hour'] = (tweets_df['created_at'].dt.tz_localize('UTC')\n",
    "                                                .dt.tz_convert('CET')\n",
    "                                                .dt.hour)\n",
    "    # Tweets are considered in work hours if they were made between 8 and 18\n",
    "    # outside of the week-end (weekday goes from 0 (Monday) to 6 (Sunday)).\n",
    "    tweets_df['isin_workhour'] = (\n",
    "        (tweets_df['hour'] > 7) \n",
    "        & (tweets_df['hour'] < 18)\n",
    "        & (tweets_df['created_at'].dt.weekday < 5))\n",
    "    \n",
    "    groupby_cols = ['uid', 'place_id', 'isin_workhour']\n",
    "    # We first count the number of times a user has tweeted in each place inside\n",
    "    # and outside work hours.\n",
    "    user_places_habits = tweets_counts.increment_counts(\n",
    "        user_places_habits, tweets_df, groupby_cols)\n",
    "    \n",
    "    has_gps = tweets_df['area'] == 0\n",
    "    tweets_cells_df = geopd.sjoin(tweets_df.loc[has_gps], cells_in_area_df, \n",
    "        op='within', rsuffix='cell', how='inner')\n",
    "    # geopd adds an underscore by itself to the suffix\n",
    "    tweets_cells_df = tweets_cells_df.rename(columns={'index_cell': 'cell_id'})\n",
    "    groupby_cols = ['uid', 'cell_id', 'isin_workhour']\n",
    "    # Then we do the same thing except in each cell, using the tweets with\n",
    "    # coordinates.\n",
    "    user_cells_habits = tweets_counts.increment_counts(user_cells_habits,\n",
    "        tweets_cells_df, groupby_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we took number of speakers, whether they're multilingual or monolingual, if they speak a language, they count as one in that language's count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possibility: pass the places counts to cells counts here, and then do the whole residence attribution solely\n",
    "on a cell basis. Problem: user tags himself in the same city all the time, overlapping multiple cells: we'll have more than one cell with approximately the same count, and one cells takes all in the end. Typical 'winner takes all' problem in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the total number of tweets of each user, in order to be able\n",
    "# to calculate\n",
    "user_counts = user_places_habits.groupby('uid')['count'].sum().rename('user_count')\n",
    "user_home_cell = user_cells_habits.join(user_counts, how='inner')\n",
    "user_home_cell['prop_in_cell'] = user_home_cell['count'] / user_home_cell['user_count']\n",
    "user_home_cell = (user_home_cell.loc[user_home_cell['prop_in_cell'] > relevant_th]\n",
    "                                .xs(False, level='isin_workhour')\n",
    "                                .reset_index()\n",
    "                                .sort_values(by=['uid', 'count'])\n",
    "                                .groupby('uid')['cell_id']\n",
    "                                .last())\n",
    "\n",
    "users_with_cell = user_home_cell.index.values\n",
    "user_home_place = uagg.get_residence(user_places_habits, place_id_col='place_id')\n",
    "user_only_place = user_home_place.drop(users_with_cell)\n",
    "places_counts = user_only_place.to_frame().groupby('place_id').size().rename('count')\n",
    "cells_counts = user_home_cell.to_frame().groupby('cell_id').size().rename('total_count')\n",
    "\n",
    "places_langs_counts = (user_langs_agg.join(user_only_place, how='inner')\n",
    "                                     .groupby(['cld_lang', 'place_id'])\n",
    "                                     .size()\n",
    "                                     .rename('count'))\n",
    "cells_langs_counts = (user_langs_agg.join(user_home_cell, how='inner')\n",
    "                                     .groupby(['cld_lang', 'cell_id'])\n",
    "                                     .size()\n",
    "                                     .rename('count'))\n",
    "\n",
    "# We initialize cell_plot_df with the counts from the users for which we could\n",
    "# only find a place for residence\n",
    "cell_plot_df = places_to_cells.get_counts(\n",
    "    places_counts, places_langs_counts, places_geodf,\n",
    "    cells_in_area_df, plot_langs_dict, xy_proj=xy_proj)\n",
    "# Then we add the total counts for the users with a cell of residence.\n",
    "cell_plot_df = tweets_counts.increment_join(cell_plot_df, cells_counts, \n",
    "                             count_col='total_count')\n",
    "\n",
    "cell_plot_df = cell_plot_df.loc[cell_plot_df['total_count'] > 0]\n",
    "\n",
    "for plot_lang, lang_dict in plot_langs_dict.items():\n",
    "    lang_count_col = lang_dict['count_col']\n",
    "    cells_lang_counts = cells_langs_counts.xs(plot_lang).rename(lang_count_col)\n",
    "    # And then we increment the counts per language with these users.\n",
    "    cell_plot_df = tweets_counts.increment_join(cell_plot_df, cells_lang_counts, \n",
    "                             count_col=lang_count_col)\n",
    "    \n",
    "    lang_prop_col = lang_dict['prop_col']\n",
    "    level_lang_label = user_level_label.format(lang_dict['readable'])\n",
    "    sum_lang = cell_plot_df[lang_count_col].sum()\n",
    "    print(f'There are {sum_lang:.0f} {level_lang_label}.')\n",
    "    cell_plot_df[lang_prop_col] = (cell_plot_df[lang_count_col]\n",
    "                                   / cell_plot_df['total_count'])\n",
    "\n",
    "cell_plot_df['cell_id'] = cell_plot_df.index\n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df.to_file(cell_data_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_size = 10000\n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_lang, plot_dict in plot_langs_dict.items():\n",
    "    count_lang_col = plot_dict['count_col']\n",
    "    readable_lang = plot_dict['readable']\n",
    "    save_path = os.path.join(fig_dir, 'counts',\n",
    "        f'users_counts_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Distribution of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['count_label']\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='Purples')\n",
    "    ax_count = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=count_lang_col, save_path=save_path, \n",
    "        show=False, log_scale=True, title=plot_title, cbar_label=cbar_label,\n",
    "        xy_proj=xy_proj, **plot_kwargs)\n",
    "    \n",
    "    prop_lang_col = plot_dict['prop_col']\n",
    "    save_path = os.path.join(fig_dir, 'prop',\n",
    "        f'users_prop_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Predominance of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['prop_label']\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax_prop = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=prop_lang_col, save_path=save_path, \n",
    "        title=plot_title, cbar_label=cbar_label, vmax=1, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(fig_dir, \n",
    "            f'users_prop_cc={cc}_cell_size={cell_size}m.html')\n",
    "\n",
    "fig = grid_viz.plot_interactive(cell_plot_df, shape_df, plot_langs_dict,\n",
    "    save_path=save_path, plotly_renderer='iframe_connected', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "272px",
    "width": "238px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
