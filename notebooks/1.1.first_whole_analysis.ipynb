{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "US: Louisiana (fr and es), and Florida, Texas, new mexico, Nevada, Arizona and California (es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "any function that's passed to a multiprocessing function must be defined globally, even the callback function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Reload all src modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cProfile\n",
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    import cld3\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "import pycld2\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import datetime\n",
    "import src.utils.geometry as geo\n",
    "import src.utils.places_to_cells as places_to_cells\n",
    "import src.utils.join_and_count as join_and_count\n",
    "import src.utils.make_config as make_config\n",
    "import src.data.shp_extract as shp_extract\n",
    "import src.data.text_process as text_process\n",
    "import src.data.access as data_access\n",
    "import src.data.user_filters as ufilters\n",
    "import src.data.user_agg as uagg\n",
    "import src.data.metrics as metrics\n",
    "import src.data.process as data_process\n",
    "import src.data.cells_results as cells_results\n",
    "import src.visualization.grid_viz as grid_viz\n",
    "import src.visualization.helpers as helpers_viz\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_dir_path = os.environ['DATA_DIR']\n",
    "tweets_files_format = 'tweets_{}_{}_{}.json.gz'\n",
    "places_files_format = 'places_{}_{}_{}.json.gz'\n",
    "ssh_domain = os.environ['IFISC_DOMAIN']\n",
    "ssh_username = os.environ['IFISC_USERNAME']\n",
    "fig_dir = os.path.join('..', 'reports', 'figures')\n",
    "project_data_dir = os.path.join('..', 'data')\n",
    "external_data_dir = os.path.join(project_data_dir, 'external')\n",
    "interim_data_dir = os.path.join(project_data_dir, 'interim')\n",
    "processed_data_dir = os.path.join(project_data_dir, 'processed')\n",
    "cell_data_path_format = os.path.join(processed_data_dir,\n",
    "                                     '{}_cell_data_cc={}_cell_size={}m.geojson')\n",
    "latlon_proj = 'epsg:4326'\n",
    "LANGS_DICT = dict([(lang[1],lang[0].lower().capitalize())\n",
    "                   for lang in pycld2.LANGUAGES])\n",
    "\n",
    "country_codes = ('BE', 'BO', 'CA', 'CH', 'EE', 'ES', 'FR', 'HK', 'ID', 'LT', \n",
    "                 'LV', 'MY', 'PE', 'RO', 'SG', 'TN', 'UA')\n",
    "with open(os.path.join(external_data_dir, 'countries.json')) as f:\n",
    "    countries_study_data = json.load(f)\n",
    "with open(os.path.join(external_data_dir, 'langs_agg.json')) as f:\n",
    "    langs_agg_dict = json.load(f)\n",
    "\n",
    "# Country-specific parameters\n",
    "cc = 'CH'\n",
    "region = None\n",
    "# region = 'Quebec'\n",
    "# region = 'CataluÃ±a'\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "country_name = area_dict['readable']    \n",
    "cc_fig_dir = os.path.join(fig_dir, cc)\n",
    "if not os.path.exists(cc_fig_dir):\n",
    "    os.makedirs(os.path.join(cc_fig_dir, 'counts'))\n",
    "    os.makedirs(os.path.join(cc_fig_dir, 'prop'))\n",
    "xy_proj = area_dict['xy_proj']\n",
    "cc_timezone = area_dict['timezone']\n",
    "plot_langs_list = area_dict['local_langs']\n",
    "min_poly_area = area_dict.get('min_poly_area')\n",
    "max_place_area = area_dict.get('max_place_area') or 1e9\n",
    "valid_uids_path = os.path.join(interim_data_dir, f'valid_uids_{cc}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Places, area and grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict, xy_proj=xy_proj, \n",
    "                             min_area=min_poly_area)\n",
    "shape_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Places can be a point too -> treat them like tweets with coords in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "places_files_paths = [\n",
    "    os.path.join(data_dir_path, places_files_format.format(2015, 2018, cc)),\n",
    "    os.path.join(data_dir_path, places_files_format.format(2019, 2019, cc))]\n",
    "all_raw_places_df = []\n",
    "for file in places_files_paths:\n",
    "    raw_places_df = data_access.return_json(file,\n",
    "        ssh_domain=ssh_domain, ssh_username=ssh_username, compression='gzip')\n",
    "    all_raw_places_df.append(\n",
    "        raw_places_df[['id', 'bounding_box', 'name', 'place_type']])\n",
    "# We drop the duplicate places (based on their ID)\n",
    "places_df = pd.concat(all_raw_places_df).drop_duplicates(subset='id')\n",
    "places_geodf, places_in_xy = geo.make_places_geodf(places_df, shape_df,\n",
    "                                                   xy_proj=xy_proj)\n",
    "places_geodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cell_size = 10000\n",
    "cells_df, cells_in_area_df, Nx, Ny = geo.create_grid(\n",
    "    shape_df, cell_size, xy_proj=xy_proj, intersect=True)\n",
    "grid_test_df = cells_in_area_df.copy()\n",
    "grid_test_df['metric'] = 1\n",
    "save_path = os.path.join(cc_fig_dir, f'grid_cc={cc}_cell_size={cell_size}m.pdf')\n",
    "plot_kwargs = dict(alpha=0.7, edgecolor='w', linewidths=0.5, cmap='plasma')\n",
    "ax = grid_viz.plot_grid(grid_test_df, shape_df, metric_col='metric', show=True, \n",
    "                        save_path=save_path, xy_proj=xy_proj, **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tweets_files_paths = [\n",
    "    os.path.join(data_dir_path, tweets_files_format.format(2015, 2018, cc)),\n",
    "    os.path.join(data_dir_path, tweets_files_format.format(2019, 2019, cc))]\n",
    "\n",
    "tweets_access_res = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def profile_pre_process(tweets_file_path, chunk_start, chunk_size):\n",
    "    cProfile.runctx(\n",
    "        '''data_access.read_data(\n",
    "        tweets_file_path, chunk_start, chunk_size, dfs_to_join=[places_geodf])''', \n",
    "        globals(), locals())\n",
    "\n",
    "tweets_access_res = []\n",
    "def collect_tweets_access_res(res):\n",
    "    global tweets_access_res\n",
    "    if res.shape[0] > 0:\n",
    "        tweets_access_res.append(res)\n",
    "    \n",
    "pool = mp.Pool(8)\n",
    "for file_path in tweets_files_paths:\n",
    "    for chunk_start, chunk_size in data_access.chunkify(\n",
    "            file_path, size=1e9, ssh_domain=ssh_domain, \n",
    "            ssh_username=ssh_username):\n",
    "        args = (file_path, chunk_start, chunk_size)\n",
    "        kwargs = {'cols': ['text', 'id', 'lang', 'place_id', 'coordinates', \n",
    "                           'uid', 'created_at', 'source'],\n",
    "                  'dfs_to_join': [places_geodf]}\n",
    "        pool.apply_async(\n",
    "            data_access.read_data, args, kwargs, callback=collect_tweets_access_res)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "tweets_access_res = data_process.post_multi(tweets_access_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tweeted_months = None\n",
    "tweets_pb_months = None\n",
    "first_day = datetime.datetime(year=2015, month=1, day=1)\n",
    "for res in tweets_access_res:\n",
    "    tweets_df = res.copy()\n",
    "    tweets_df = tweets_df.loc[tweets_df['created_at'] > first_day]\n",
    "    tweets_df['month'] = tweets_df['created_at'].dt.to_period('M')\n",
    "    has_gps = tweets_df['coordinates'].notnull()\n",
    "    geometry = tweets_df.loc[has_gps, 'coordinates'].apply(\n",
    "        lambda x: Point(x['coordinates']))\n",
    "    tweets_coords = geopd.GeoSeries(geometry, crs=latlon_proj, \n",
    "                                    index=tweets_df.loc[has_gps].index)\n",
    "    tweets_df = tweets_df.join(places_geodf, on='place_id', how='left')\n",
    "    coords_in_place = tweets_coords.within(\n",
    "        geopd.GeoSeries(tweets_df.loc[has_gps, 'geometry']))\n",
    "    \n",
    "    tweeted_months = join_and_count.increment_counts(\n",
    "        tweeted_months, tweets_df, ['month'])\n",
    "    tweets_pb_months = join_and_count.increment_counts(tweets_pb_months, \n",
    "        tweets_df.loc[has_gps].loc[~coords_in_place], ['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "months_counts = tweeted_months.join(tweets_pb_months, rsuffix='_pb', how='left')\n",
    "months_counts['prop'] = months_counts['count_pb'] / months_counts['count']\n",
    "ax = months_counts['prop'].plot.bar()\n",
    "ticks = np.arange(0,60,5)\n",
    "tick_labels = ax.get_xticklabels()\n",
    "_ = ax.set_xticks(ticks)\n",
    "_ = ax.set_xticklabels([tick_labels[i] for i in ticks])\n",
    "_ = ax.set_ylabel('proportion')\n",
    "_ = ax.set_title('Proportion of tweets with coords outside of place')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Filtering out users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Filters: user-based imply a loop over all the raw_tweets_df, and must be applied before getting tweets_lang_df and even tweets_loc_df, because these don't interest us at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This filter requires us to loop over all files and aggregate the results to get the valid UIDs out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if tweets_access_res is None:\n",
    "    def get_df_fun(arg0):\n",
    "        return data_access.read_json_wrapper(*arg0)\n",
    "else:\n",
    "    def get_df_fun(arg0):\n",
    "        return arg0\n",
    "\n",
    "def chunk_users_months(df_access, get_df_fun, places_geodf,\n",
    "                       cols=None, ref_year=2015):\n",
    "    raw_tweets_df = get_df_fun(df_access)\n",
    "    raw_tweets_df = data_access.filter_df(\n",
    "        raw_tweets_df, cols=cols, dfs_to_join=[places_geodf])\n",
    "    months_counts = uagg.users_months(raw_tweets_df, ref_year=ref_year)\n",
    "    return months_counts\n",
    "\n",
    "users_months_res = []\n",
    "def collect_users_months_res(res):\n",
    "    global users_months_res\n",
    "    if res.shape[0] > 0:\n",
    "        users_months_res.append(res)\n",
    "\n",
    "pool = mp.Pool(8)\n",
    "for df_access in data_access.yield_tweets_access(\n",
    "        tweets_files_paths, tweets_res=tweets_access_res):\n",
    "    args = (df_access, get_df_fun, places_geodf)\n",
    "    kwargs = {'cols': ['id', 'uid', 'created_at']}\n",
    "    pool.apply_async(\n",
    "        chunk_users_months, args, kwargs, \n",
    "        callback=collect_users_months_res, error_callback=print)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "tweeted_months_users = join_and_count.init_counts(['uid', 'month'])\n",
    "for res in users_months_res:\n",
    "    tweeted_months_users = join_and_count.increment_join(tweeted_months_users, \n",
    "                                                         res)\n",
    "        \n",
    "tweeted_months_users = tweeted_months_users['count']\n",
    "total_nr_users = len(tweeted_months_users.index.levels[0])\n",
    "print(f'In total, there are {total_nr_users} distinct users in the whole dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "local_uids = ufilters.consec_months(tweeted_months_users)\n",
    "bot_uids = ufilters.bot_activity(tweeted_months_users)\n",
    "# We have local_uids: index of uids with a column full of True, and bot_uids:\n",
    "# index of uids with a column full of False. When we multiply them, the uids\n",
    "# in local_uids which are not in bot_uids are assigned NaN, and the ones which \n",
    "# are in bot_uids are assigned False. When we convert to the boolean type,\n",
    "# the NaNs turn to True.\n",
    "valid_uids = (local_uids * bot_uids).astype('bool').rename('valid')\n",
    "valid_uids = valid_uids.loc[valid_uids]\n",
    "print(f'This leaves us with {len(valid_uids)} valid users in the whole dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Then we have to loop over all files once again to apply the speed filter, which is expensive, thus done last (we thus benefit from having some users already filtered out, so smaller tweets dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if tweets_access_res is None:\n",
    "    def get_df_fun(arg0):\n",
    "        return data_access.read_json_wrapper(*arg0)\n",
    "else:\n",
    "    def get_df_fun(arg0):\n",
    "        return arg0\n",
    "\n",
    "def speed_filter(df_access, get_df_fun, valid_uids, places_in_xy, max_distance,\n",
    "                 cols=None):\n",
    "    tweets_df = get_df_fun(df_access)\n",
    "    tweets_df = data_access.filter_df(\n",
    "        tweets_df, cols=cols, dfs_to_join=[places_in_xy, valid_uids])\n",
    "    too_fast_uids = ufilters.too_fast(tweets_df, places_in_xy, max_distance)\n",
    "    return too_fast_uids\n",
    "\n",
    "area_bounds = shape_df.to_crs(xy_proj).geometry.iloc[0].bounds\n",
    "# Get an upper limit of the distance that can be travelled inside the area\n",
    "max_distance = np.sqrt((area_bounds[0]-area_bounds[2])**2 \n",
    "                       + (area_bounds[1]-area_bounds[3])**2)\n",
    "cols = ['uid', 'created_at', 'place_id', 'coordinates']\n",
    "\n",
    "too_fast_uids_list = []\n",
    "def collect_too_fast_uids_list(res):\n",
    "    global too_fast_uids_list\n",
    "    if res.shape[0] > 0:\n",
    "        too_fast_uids_list.append(res)\n",
    "        \n",
    "pool = mp.Pool(8)\n",
    "for df_access in data_access.yield_tweets_access(\n",
    "        tweets_files_paths, tweets_res=tweets_access_res):\n",
    "    args = (df_access, get_df_fun,\n",
    "            valid_uids, places_geodf, max_distance)\n",
    "    kwargs = {'cols': cols}\n",
    "    pool.apply_async(\n",
    "        speed_filter, args, kwargs, callback=collect_too_fast_uids_list,\n",
    "        error_callback=print)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "too_fast_uids_series = pd.Series([])\n",
    "too_fast_uids_series.index.name = 'uid'\n",
    "for too_fast_uids in too_fast_uids_list:\n",
    "    too_fast_uids_series = (too_fast_uids_series * too_fast_uids).fillna(False)\n",
    "print(f'In total, there are {len(too_fast_uids_series)} too fast users left to '\n",
    "      'filter out in the whole dataset.')\n",
    "\n",
    "valid_uids = (valid_uids * too_fast_uids_series).astype('bool').rename('valid')\n",
    "valid_uids = valid_uids.loc[valid_uids]\n",
    "print(f'This leaves us with {len(valid_uids)} valid users in the whole dataset.')\n",
    "valid_uids.to_csv(valid_uids_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We don't filter out tweets with a useless place (one too large) here, because these tweets can still be useful for language detection. So this filter is only applied later on. Similarly, we keep tweets with insufficient text to make a reliable language detection, because they can still be useful for residence attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "valid_uids = pd.read_csv(valid_uids_path, index_col='uid', header=0)\n",
    "\n",
    "if tweets_access_res is None:\n",
    "    def get_df_fun(arg0):\n",
    "        return data_access.read_json_wrapper(*arg0)\n",
    "else:\n",
    "    def get_df_fun(arg0):\n",
    "        return arg0\n",
    "    \n",
    "tweets_process_res = []\n",
    "def collect_tweets_process_res(res):\n",
    "    global tweets_process_res\n",
    "    if res.shape[0] > 0:\n",
    "        tweets_process_res.append(res)\n",
    "        \n",
    "pool = mp.Pool(8)\n",
    "for df_access in data_access.yield_tweets_access(\n",
    "        tweets_files_paths, tweets_res=tweets_access_res):\n",
    "    args = (df_access, get_df_fun,\n",
    "            valid_uids, places_geodf, langs_agg_dict)\n",
    "    kwargs = {'min_nr_words': 4, 'cld': 'pycld2'}\n",
    "    pool.apply_async(\n",
    "        data_process.process, args, kwargs, callback=collect_tweets_process_res,\n",
    "        error_callback=print)\n",
    "pool.close()\n",
    "pool.join()\n",
    "    \n",
    "tweets_process_res = data_process.post_multi(tweets_process_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "# Study at the tweet level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "## Make tweet counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweet_level_label = 'tweets in {}'\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, tweet_level_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "Why sjoin so slow? It tests on every cell, even though it's exclusive: if one cell matches no other will. Solution: loop over cells, ordered by the counts obtained from places, and stop at first match, will greatly reduce the number of 'within' operations -> update: doesn't seem possible, deleting from spatial index is extremely slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_langs_counts(tweets_lang_df, max_place_area, cells_in_area_df):\n",
    "    tweets_df = tweets_lang_df.copy()\n",
    "    relevant_area_mask = tweets_df['area'] < max_place_area\n",
    "    tweets_df = tweets_df.loc[relevant_area_mask]\n",
    "    # The following mask accounts for both tweets with GPS coordinates and\n",
    "    # tweets within places which are a point.\n",
    "    has_gps = tweets_df['area'] == 0\n",
    "    # Here the tweets with coordinates outside the grid are out, because of the\n",
    "    # inner join\n",
    "    tweets_cells_df = geopd.sjoin(tweets_df.loc[has_gps], cells_in_area_df,\n",
    "        op='within', rsuffix='cell', how='inner')\n",
    "    nr_out_tweets =  len(tweets_df.loc[has_gps]) - len(tweets_cells_df)\n",
    "    print(f'{nr_out_tweets} tweets have been found outside of the grid and'\n",
    "         ' filtered out as a result.')\n",
    "    tweets_places_df = tweets_df.loc[~has_gps]\n",
    "    return tweets_cells_df, tweets_places_df\n",
    "    \n",
    "with mp.Pool(8) as pool:\n",
    "    map_parameters = [(res, max_place_area, cells_in_area_df) \n",
    "                      for res in tweets_process_res]\n",
    "    print('entering the loop')\n",
    "    tweets_pre_cell_res = (\n",
    "        pool.starmap_async(get_langs_counts, map_parameters).get())\n",
    "\n",
    "cells_langs_counts = None\n",
    "places_langs_counts = None\n",
    "\n",
    "for res in tweets_pre_cell_res:\n",
    "    tweets_cells_df = res[0]\n",
    "    tweets_places_df = res[1]\n",
    "    groupby_cols = ['cld_lang', 'cell_id']\n",
    "    cells_langs_counts = join_and_count.increment_counts(\n",
    "        cells_langs_counts, tweets_cells_df, groupby_cols)\n",
    "    groupby_cols = ['cld_lang', 'place_id']\n",
    "    places_langs_counts = join_and_count.increment_counts(\n",
    "        places_langs_counts, tweets_places_df, groupby_cols)\n",
    "\n",
    "places_langs_counts = places_langs_counts['count']\n",
    "places_counts = (places_langs_counts.groupby('place_id')\n",
    "                                   .sum()\n",
    "                                   .rename('total_count')\n",
    "                                   .to_frame())\n",
    "cells_langs_counts = cells_langs_counts['count']\n",
    "cells_counts = (cells_langs_counts.groupby('cell_id')\n",
    "                                  .sum()\n",
    "                                  .rename('total_count')\n",
    "                                  .to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "Places -> cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We count the number of users speaking a local language in each cell and place \n",
    "# of residence.\n",
    "local_langs = [lang for lang in plot_langs_dict]\n",
    "places_local_counts = places_langs_counts.reset_index(level='cld_lang')\n",
    "local_langs_mask = places_local_counts['cld_lang'].isin(local_langs)\n",
    "places_local_counts = (places_local_counts.loc[local_langs_mask]\n",
    "                                          .groupby('place_id')['count']\n",
    "                                          .sum()\n",
    "                                          .rename('local_count'))\n",
    "places_counts = places_counts.join(places_local_counts, how='left')\n",
    "\n",
    "cells_local_counts = cells_langs_counts.reset_index(level='cld_lang')\n",
    "local_langs_mask = cells_local_counts['cld_lang'].isin(local_langs)\n",
    "cells_local_counts = (cells_local_counts.loc[local_langs_mask]\n",
    "                                        .groupby('cell_id')['count']\n",
    "                                        .sum()\n",
    "                                        .rename('local_count'))\n",
    "cells_counts = cells_counts.join(cells_local_counts, how='left')\n",
    "\n",
    "cell_plot_df = places_to_cells.get_counts(\n",
    "    places_counts, places_langs_counts, places_geodf,\n",
    "    cells_in_area_df, plot_langs_dict)\n",
    "\n",
    "# We add the counts from the tweets with coordinates\n",
    "cell_plot_df = join_and_count.increment_join(\n",
    "    cell_plot_df, cells_counts['total_count'], count_col='total_count')\n",
    "cell_plot_df = join_and_count.increment_join(\n",
    "    cell_plot_df, cells_counts['local_count'], count_col='local_count')\n",
    "cell_plot_df = cell_plot_df.loc[cell_plot_df['total_count'] > 0]\n",
    "\n",
    "for plot_lang, lang_dict in plot_langs_dict.items():\n",
    "    lang_count_col = lang_dict['count_col']\n",
    "    cells_lang_counts = cells_langs_counts.xs(plot_lang).rename(lang_count_col)\n",
    "    cell_plot_df = join_and_count.increment_join(\n",
    "        cell_plot_df, cells_lang_counts, count_col=lang_count_col)\n",
    "    \n",
    "    level_lang_label = tweet_level_label.format(lang_dict['readable'])\n",
    "    sum_lang = cell_plot_df[lang_count_col].sum()\n",
    "    print(f'There are {sum_lang:.0f} {level_lang_label}.')\n",
    "    \n",
    "cell_plot_df['cell_id'] = cell_plot_df.index\n",
    "cell_data_path = cell_data_path_format.format('tweets', cc, cell_size)\n",
    "cell_plot_df.to_file(cell_data_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cell_size = 20000\n",
    "cell_data_path = cell_data_path_format.format('tweets', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(cell_plot_df, plot_langs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for plot_lang, plot_dict in plot_langs_dict.items():\n",
    "    count_lang_col = plot_dict['count_col']\n",
    "    readable_lang = plot_dict['readable']\n",
    "    save_path = os.path.join(cc_fig_dir, 'counts',\n",
    "        f'tweet_counts_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Distribution of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['count_label']\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='Purples')\n",
    "    ax_count = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=count_lang_col, save_path=save_path, \n",
    "        show=False, log_scale=True, title=plot_title, cbar_label=cbar_label,\n",
    "        xy_proj=xy_proj, **plot_kwargs)\n",
    "    \n",
    "    prop_lang_col = plot_dict['prop_col']\n",
    "    save_path = os.path.join(cc_fig_dir, 'prop',\n",
    "        f'tweets_prop_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = '{} predominance in {}'.format(readable_lang, country_name)\n",
    "    cbar_label = plot_dict['prop_label']\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax_prop = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=prop_lang_col, save_path=save_path, \n",
    "        title=plot_title, cbar_label=cbar_label, vmin=0, vmax=1, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join(cc_fig_dir, \n",
    "            f'tweets_prop_cc={cc}_cell_size={cell_size}m.html')\n",
    "prop_dict = {'name': 'prop', 'readable': 'proportion', 'vmin': 0, 'vmax': 1}\n",
    "fig = grid_viz.plot_interactive(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, prop_dict,\n",
    "    save_path=save_path, plotly_renderer='iframe_connected', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Study at the user level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Users who have tagged their tweets with gps coordinates seem to do it regularly, as the median of the proportion of tweets they geo tag is at more than 75% on the first chunk -> it's worth it to try and get their cell of residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "a = tweets_process_res[0].copy()\n",
    "a['has_gps'] = a['area'] == 0\n",
    "gps_uids = a.loc[a['has_gps'], 'uid'].unique()\n",
    "a = a.loc[a['uid'].isin(gps_uids)].groupby(['uid', 'has_gps']).size().rename('count').to_frame()\n",
    "a = a.join(a.groupby('uid')['count'].sum().rename('sum'))\n",
    "b = a.reset_index()\n",
    "b = b.loc[b['has_gps']]\n",
    "b['ratio'] = b['count'] / b['sum']\n",
    "b['ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If there's one or more cells where a user tweeted in proportion more than relevant_th of the time, we take among these cells the one where they tweeted the most outside work hours. Otherwise, we take the relevant place where they tweeted the most outside work hours, or we default to the place where they tweeted the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "user_level_label = '{}-speaking users'\n",
    "lang_relevant_prop = 0.1\n",
    "lang_relevant_count = 5\n",
    "cell_relevant_th = 0.1\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If valid_uids is already generated, we only loop once over the tweets df and do the whole processing in one go on each file, thus keeping very little in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "valid_uids = pd.read_csv(valid_uids_path, index_col='uid', header=0)\n",
    "cells_df_list = [cells_in_area_df]\n",
    "\n",
    "if tweets_access_res is None:\n",
    "    def get_df_fun(arg0):\n",
    "        return data_access.read_json_wrapper(*arg0)\n",
    "else:\n",
    "    def get_df_fun(arg0):\n",
    "        return arg0\n",
    "    \n",
    "user_agg_res = []\n",
    "def collect_user_agg_res(res):\n",
    "    global user_agg_res\n",
    "    user_agg_res.append(res)\n",
    "        \n",
    "pool = mp.Pool(8)\n",
    "for df_access in data_access.yield_tweets_access(tweets_files_paths):\n",
    "    args = (df_access, get_df_fun, valid_uids, places_geodf, langs_agg_dict,\n",
    "            cells_df_list, max_place_area, cc_timezone)\n",
    "    kwargs = {'min_nr_words': 4, 'cld': 'pycld2'}\n",
    "    pool.apply_async(\n",
    "        uagg.get_lang_loc_habits, args, kwargs, callback=collect_user_agg_res,\n",
    "        error_callback=print)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "user_langs_counts = join_and_count.init_counts(['uid', 'cld_lang'])\n",
    "user_cells_habits = join_and_count.init_counts(['uid', 'cell_id', \n",
    "                                                'isin_workhour'])\n",
    "user_places_habits = join_and_count.init_counts(['uid', 'place_id', \n",
    "                                                 'isin_workhour'])\n",
    "for lang_res, cell_res, place_res in user_agg_res:\n",
    "    user_langs_counts = join_and_count.increment_join(user_langs_counts, \n",
    "                                                      lang_res)\n",
    "    user_cells_habits = join_and_count.increment_join(user_cells_habits, \n",
    "                                                      cell_res[0])\n",
    "    user_places_habits = join_and_count.increment_join(user_places_habits, \n",
    "                                                       place_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Language(s) attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "very few users are actually filtered out by language attribution: not more worth it to generate user_langs_counts, user_cells_habits and user_places_habits inside of tweets_lang_df loop, so as to drop tweets_langs_df, and only return these user level, lightweight DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " Here we get rid of users whose language we couldn't identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Residence attribution is the longest to run, and by a long shot, so we'll start\n",
    "# with language to filter out uids in tweets_df before doing it\n",
    "groupby_cols = ['uid', 'cld_lang']\n",
    "user_langs_counts = None\n",
    "for res in tweets_process_res:\n",
    "    tweets_lang_df = res.copy()\n",
    "    # Here we don't filter out based on max_place_area, because these tweets\n",
    "    # are still useful for language attribution.\n",
    "    tweets_lang_df = tweets_lang_df.loc[tweets_lang_df['cld_lang'].notnull()]\n",
    "    user_langs_counts = join_and_count.increment_counts(\n",
    "        user_langs_counts, tweets_lang_df, groupby_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "user_langs_agg = uagg.get_lang_grp(user_langs_counts, area_dict,\n",
    "                                   lang_relevant_prop=lang_relevant_prop,\n",
    "                                   lang_relevant_count=lang_relevant_count,\n",
    "                                   fig_dir=fig_dir, show_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Attribute users to a group: mono, bi, tri, ... lingual\n",
    "\n",
    "Problem: need more tweets to detect multilingualism, eg users with only three tweets in the dataset are very unlikely to be detected as multilinguals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "users_ling_grp, multiling_grps = uagg.get_ling_grp(\n",
    "    user_langs_agg, area_dict, lang_relevant_prop=lang_relevant_prop,\n",
    "    lang_relevant_count=lang_relevant_count, fig_dir=fig_dir, show_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Pre-residence attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with mp.Pool(8) as pool:\n",
    "    map_parameters = [(res, cells_in_area_df,\n",
    "                       max_place_area, cc_timezone) \n",
    "                      for res in tweets_process_res]\n",
    "    print('entering the loop')\n",
    "    tweets_pre_resid_res = (\n",
    "        pool.starmap_async(data_process.prep_resid_attr, map_parameters).get())\n",
    "    \n",
    "user_places_habits = None\n",
    "user_cells_habits = None\n",
    "for res in tweets_pre_resid_res:\n",
    "    # We first count the number of times a user has tweeted in each place inside\n",
    "    # and outside work hours.\n",
    "    tweets_places_df = res[1]\n",
    "    groupby_cols = ['uid', 'place_id', 'isin_workhour']\n",
    "    user_places_habits = join_and_count.increment_counts(\n",
    "        user_places_habits, tweets_places_df, groupby_cols)\n",
    "    # Then we do the same thing except in each cell, using the tweets with\n",
    "    # coordinates.\n",
    "    tweets_cells_df = res[0]\n",
    "    groupby_cols = ['uid', 'cell_id', 'isin_workhour']\n",
    "    user_cells_habits = join_and_count.increment_counts(\n",
    "        user_cells_habits, tweets_cells_df, groupby_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Here we took number of speakers, whether they're multilingual or monolingual, if they speak a language, they count as one in that language's count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Residence attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "user_home_cell, user_only_place = uagg.get_residence(\n",
    "    user_cells_habits, user_places_habits, place_relevant_th=cell_relevant_th,\n",
    "    cell_relevant_th=cell_relevant_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Generate cell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cell_plot_df = data_process.from_users_area_and_lang(\n",
    "    cells_in_area_df, places_geodf, user_only_place,\n",
    "    user_home_cell, user_langs_agg, users_ling_grp,\n",
    "    plot_langs_dict, multiling_grps, cell_data_path_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "GeoJSON should always be in lat, lon, WGS84 to be read by external programs, so in plotly for instance we need to make sure we come back to latlon_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cell_size = 10000\n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(cell_plot_df, \n",
    "                                                     plot_langs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "prop_dict = {'name': 'prop', 'readable': 'Proportion', 'log_scale': False, \n",
    "             'vmin': 0, 'vmax': 1, 'total_count_col': 'local_count'}\n",
    "metric = prop_dict['name']\n",
    "save_path_format = os.path.join(\n",
    "    cc_fig_dir, metric, \n",
    "    f'users_{metric}_cc={cc}_grp={{grp}}_cell_size={cell_size}m.pdf')\n",
    "ax = helpers_viz.metric_grid(\n",
    "    cell_plot_df, prop_dict, shape_df, plot_langs_dict, country_name, \n",
    "    cmap='plasma', save_path_format=save_path_format, xy_proj=xy_proj, \n",
    "    min_count=0, null_color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join(cc_fig_dir, \n",
    "            f'users_prop_cc={cc}_cell_size={cell_size}m.html')\n",
    "prop_dict = {'name': 'prop', 'readable': 'Proportion', 'log_scale': False, \n",
    "             'vmin': 0, 'vmax': 1, 'total_count_col': 'local_count'}\n",
    "fig = grid_viz.plot_interactive(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, prop_dict,\n",
    "    save_path=save_path, plotly_renderer='iframe_connected', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Generate cell data files in loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In all the above, the cell size and cc are supposed constant, defined in config. Here we first assume the cell size is not constant, then the cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tweets_files_format = 'tweets_{}_{}_{}.json.gz'\n",
    "places_files_format = 'places_{}_{}_{}.json.gz'\n",
    "source_data_dir = os.environ['DATA_DIR']\n",
    "fig_dir = os.path.join('..', 'reports', 'figures')\n",
    "project_data_dir = os.path.join('..', 'data')\n",
    "external_data_dir = os.path.join(project_data_dir, 'external')\n",
    "interim_data_dir = os.path.join(project_data_dir, 'interim')\n",
    "processed_data_dir = os.path.join(project_data_dir, 'processed')\n",
    "cell_data_path_format = os.path.join(\n",
    "    processed_data_dir, '{}_cell_data_cc={}_cell_size={}m.geojson')\n",
    "\n",
    "with open(os.path.join(external_data_dir, 'countries.json')) as f:\n",
    "    countries_study_data = json.load(f)\n",
    "with open(os.path.join(external_data_dir, 'langs_agg.json')) as f:\n",
    "    langs_agg_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Cell sizes loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cc = 'CH'\n",
    "region = None\n",
    "\n",
    "tweets_files_paths = [\n",
    "    os.path.join(source_data_dir,\n",
    "                 tweets_files_format.format(year_from, year_to, cc))\n",
    "    for year_from, year_to in data_years]\n",
    "places_files_paths = [\n",
    "    os.path.join(source_data_dir,\n",
    "                 places_files_format.format(year_from, year_to, cc))\n",
    "    for year_from, year_to in data_years]\n",
    "\n",
    "valid_uids_path = os.path.join(interim_data_dir, f'valid_uids_{cc}.csv')\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "cell_sizes_list = [5000, 10000]\n",
    "user_level_label = '{}-speaking users'\n",
    "lang_relevant_prop = 0.1\n",
    "lang_relevant_count = 5\n",
    "cell_relevant_th = 0.1\n",
    "place_relevant_th = 0.1\n",
    "\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "shape_df, places_geodf, cells_df_list = geo.init_cc(\n",
    "    area_dict, cell_sizes_list, places_files_paths, project_data_dir)\n",
    "\n",
    "valid_uids = pd.read_csv(valid_uids_path, index_col='uid', header=0)\n",
    "    \n",
    "def get_df_fun(arg0):\n",
    "    return data_access.read_json_wrapper(*arg0)\n",
    "\n",
    "user_agg_res = []\n",
    "def collect_user_agg_res(res):\n",
    "    global user_agg_res\n",
    "    user_agg_res.append(res)\n",
    "\n",
    "max_place_area = area_dict.get('max_place_area') or 1e9\n",
    "cc_timezone = area_dict['timezone']\n",
    "pool = mp.Pool(8)\n",
    "for df_access in data_access.yield_tweets_access(tweets_files_paths):\n",
    "    args = (df_access, get_df_fun, valid_uids, places_geodf, langs_agg_dict,\n",
    "            cells_df_list, max_place_area, cc_timezone)\n",
    "    kwargs = {'min_nr_words': 4, 'cld': 'pycld2'}\n",
    "    pool.apply_async(\n",
    "        uagg.get_lang_loc_habits, args, kwargs, callback=collect_user_agg_res,\n",
    "        error_callback=print)\n",
    "pool.close()\n",
    "pool.join()\n",
    "cells_results.from_uagg_res(\n",
    "    user_agg_res, cells_df_list, places_geodf, plot_langs_dict, area_dict,\n",
    "    cell_data_path_format, lang_relevant_prop=lang_relevant_prop, \n",
    "    lang_relevant_count=lang_relevant_count, \n",
    "    place_relevant_th=place_relevant_th, cell_relevant_th=cell_relevant_th,\n",
    "    fig_dir=fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cells_results.from_uagg_res(\n",
    "    user_agg_res, cells_df_list, places_geodf, plot_langs_dict, area_dict,\n",
    "    cell_data_path_format, lang_relevant_prop=lang_relevant_prop, \n",
    "    lang_relevant_count=lang_relevant_count, \n",
    "    place_relevant_th=place_relevant_th, cell_relevant_th=cell_relevant_th,\n",
    "    fig_dir=fig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Countries loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cc = 'CH'\n",
    "region = None\n",
    "valid_uids_path = os.path.join(interim_data_dir, f'valid_uids_{cc}.csv')\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "cell_sizes_list = [10000]\n",
    "data_years = [(2015, 2018), (2019, 2019)]\n",
    "tweets_files_paths = [\n",
    "    os.path.join(source_data_dir,\n",
    "                 tweets_files_format.format(year_from, year_to, cc))\n",
    "    for year_from, year_to in data_years]\n",
    "places_files_paths = [\n",
    "    os.path.join(source_data_dir,\n",
    "                 places_files_format.format(year_from, year_to, cc))\n",
    "    for year_from, year_to in data_years]\n",
    "lang_relevant_prop = 0.1\n",
    "lang_relevant_count = 5\n",
    "cell_relevant_th = 0.1\n",
    "\n",
    "def get_df_fun(arg0):\n",
    "    return data_access.read_json_wrapper(*arg0)\n",
    "\n",
    "shape_df, places_geodf, cells_df_list = geo.init_cc(\n",
    "    area_dict, cell_sizes_list, places_files_paths, project_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "filters_pass_res = []\n",
    "def collect_filters_pass_res(res):\n",
    "    global filters_pass_res\n",
    "    filters_pass_res.append(res)\n",
    "\n",
    "valid_uids = ufilters.get_valid_uids(\n",
    "    places_geodf, shape_df, get_df_fun, collect_filters_pass_res,\n",
    "    filters_pass_res, tweets_files_paths, cpus=8)\n",
    "valid_uids.to_csv(valid_uids_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "valid_uids = pd.read_csv(valid_uids_path, index_col='uid', header=0)\n",
    "user_agg_res = []\n",
    "def collect_user_agg_res(res):\n",
    "    global user_agg_res\n",
    "    user_agg_res.append(res)\n",
    "    \n",
    "cells_results.from_scratch(\n",
    "    area_dict, valid_uids, places_geodf, cells_df_list,\n",
    "    tweets_files_paths, get_df_fun, collect_user_agg_res, \n",
    "    user_agg_res, langs_agg_dict, cell_data_path_format,  \n",
    "    lang_relevant_prop=0.1, lang_relevant_count=5, cell_relevant_th=0.1,\n",
    "    place_relevant_th=0.1, fig_dir=fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "272px",
    "width": "238px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 705.85,
   "position": {
    "height": "727.85px",
    "left": "256px",
    "right": "20px",
    "top": "121px",
    "width": "723px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
