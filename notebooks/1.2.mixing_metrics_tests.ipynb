{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all src modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cProfile\n",
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    import cld3\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "import pycld2\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import datetime\n",
    "import src.utils.geometry as geo\n",
    "import src.utils.places_to_cells as places_to_cells\n",
    "import src.utils.join_and_count as join_and_count\n",
    "import src.utils.make_config as make_config\n",
    "import src.data.shp_extract as shp_extract\n",
    "import src.data.text_process as text_process\n",
    "import src.data.access as data_access\n",
    "import src.data.user_filters as ufilters\n",
    "import src.data.user_agg as uagg\n",
    "import src.data.metrics as metrics\n",
    "import src.visualization.grid_viz as grid_viz\n",
    "import src.visualization.helpers as helpers_viz\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container </style>\"))\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = os.environ['DATA_DIR']\n",
    "tweets_files_format = 'tweets_{}_{}_{}.json.gz'\n",
    "places_files_format = 'places_{}_{}_{}.json.gz'\n",
    "ssh_domain = os.environ['IFISC_DOMAIN']\n",
    "ssh_username = os.environ['IFISC_USERNAME']\n",
    "project_data_dir = os.path.join('..', 'data')\n",
    "external_data_dir = os.path.join(project_data_dir, 'external')\n",
    "interim_data_dir = os.path.join(project_data_dir, 'interim')\n",
    "processed_data_dir = os.path.join(project_data_dir, 'processed')\n",
    "cell_data_path_format = os.path.join(processed_data_dir,\n",
    "                                     '{}_cell_data_cc={}_cell_size={}m.geojson')\n",
    "latlon_proj = 'epsg:4326'\n",
    "LANGS_DICT = dict([(lang[1],lang[0].lower().capitalize())\n",
    "                   for lang in pycld2.LANGUAGES])\n",
    "\n",
    "country_codes = ('BE', 'BO', 'CA', 'CH', 'EE', 'ES', 'FR', 'HK', 'ID', 'LT', \n",
    "                 'LV', 'MY', 'PE', 'RO', 'SG', 'TN', 'UA')\n",
    "with open(os.path.join(external_data_dir, 'countries.json')) as f:\n",
    "    countries_study_data = json.load(f)\n",
    "with open(os.path.join(external_data_dir, 'langs_agg.json')) as f:\n",
    "    langs_agg_dict = json.load(f)\n",
    "\n",
    "# Country-specific parameters\n",
    "cc = 'CH'\n",
    "region = None\n",
    "# region = 'Cataluña'\n",
    "if region:\n",
    "    area_dict = countries_study_data[cc]['regions'][region]\n",
    "else:\n",
    "    area_dict = countries_study_data[cc]\n",
    "    \n",
    "fig_dir = os.path.join('..', 'reports', 'figures', cc)\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(os.path.join(fig_dir, 'counts'))\n",
    "    os.makedirs(os.path.join(fig_dir, 'prop'))\n",
    "xy_proj = area_dict['xy_proj']\n",
    "cc_timezone = area_dict['timezone']\n",
    "plot_langs_list = area_dict['local_langs']\n",
    "min_poly_area = area_dict.get('min_poly_area') or 0.1\n",
    "max_place_area = area_dict.get('max_place_area') or 1e9 # linked to cell size and places data\n",
    "valid_uids_path = os.path.join(interim_data_dir, f'valid_uids_{cc}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of bots, company account (eg careerarc, tweetmyjobs). If can't distinguish companies by source like careerarc, then how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Places, area and grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "shape_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Places can be a point too -> treat them like tweets with coords in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "places_files_paths = [\n",
    "    os.path.join(data_dir_path, places_files_format.format(2015, 2018, cc))]\n",
    "all_raw_places_df = []\n",
    "for file in places_files_paths:\n",
    "    raw_places_df = data_access.return_json(file,\n",
    "        ssh_domain=ssh_domain, ssh_username=ssh_username, compression='gzip')\n",
    "    all_raw_places_df.append(\n",
    "        raw_places_df[['id', 'bounding_box', 'name', 'place_type']])\n",
    "    \n",
    "# We drop the duplicate places (based on their ID)\n",
    "places_df = pd.concat(all_raw_places_df).drop_duplicates(subset='id')\n",
    "places_geodf, places_in_xy = geo.make_places_geodf(places_df, shape_df,\n",
    "                                                   xy_proj=xy_proj)\n",
    "places_geodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cell_size = 5000\n",
    "max_place_area = 1e9 # linked to cell size and places data\n",
    "cells_df, cells_in_area_df, Nx, Ny = geo.create_grid(\n",
    "    shape_df, cell_size, xy_proj=xy_proj, intersect=True)\n",
    "grid_test_df = cells_in_area_df.copy()\n",
    "grid_test_df['metric'] = 1\n",
    "save_path = os.path.join(fig_dir, f'grid_cc={cc}_cell_size={cell_size}m.pdf')\n",
    "plot_kwargs = dict(alpha=0.7, edgecolor='w', linewidths=0.5, cmap='plasma')\n",
    "ax = grid_viz.plot_grid(grid_test_df, shape_df, metric_col='metric', show=True, \n",
    "                        save_path=save_path, xy_proj=xy_proj, **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweets_files_paths = [\n",
    "    os.path.join(data_dir_path, tweets_files_format.format(2015, 2018, cc))]\n",
    "\n",
    "def read_data(tweets_file_path, chunk_start, chunk_size, places_geodf):\n",
    "    raw_tweets_df = data_access.read_json_wrapper(\n",
    "        tweets_file_path, chunk_start, chunk_size, ssh_domain=ssh_domain,\n",
    "        ssh_username=ssh_username)\n",
    "    og_cols = raw_tweets_df.columns.values\n",
    "    raw_tweets_df = raw_tweets_df.join(places_geodf, on='place_id', how='inner')\n",
    "    raw_tweets_df = raw_tweets_df.loc[:, og_cols]\n",
    "    return raw_tweets_df\n",
    "\n",
    "def profile_pre_process(tweets_file_path, chunk_start, chunk_size):\n",
    "    cProfile.runctx(\n",
    "        'read_data(tweets_file_path, chunk_start, chunk_size, places_geodf)', \n",
    "        globals(), locals())\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    tweets_access_res = []\n",
    "    for file_path in tweets_files_paths:\n",
    "        for chunk_start, chunk_size in data_access.chunkify(\n",
    "                file_path, size=1e9, ssh_domain=ssh_domain, \n",
    "                ssh_username=ssh_username):\n",
    "            tweets_access_res.append(pool.apply_async(\n",
    "                read_data, (file_path, chunk_start, chunk_size, places_geodf)))\n",
    "    \n",
    "    # This is mandatory so that the pool doesn't stop working until every\n",
    "    # chunk has been processed.\n",
    "    for res in tweets_access_res:\n",
    "        res.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweeted_months = None\n",
    "tweets_pb_months = None\n",
    "first_day = datetime.datetime(year=2015, month=1, day=1)\n",
    "for res in tweets_access_res:\n",
    "    tweets_df = res.get().copy()\n",
    "    tweets_df = tweets_df.loc[tweets_df['created_at'] > first_day]\n",
    "    tweets_df['month'] = tweets_df['created_at'].dt.to_period('M')\n",
    "    has_gps = tweets_df['coordinates'].notnull()\n",
    "    geometry = tweets_df.loc[has_gps, 'coordinates'].apply(lambda x: Point(x['coordinates']))\n",
    "    tweets_coords = geopd.GeoSeries(geometry, crs=latlon_proj, index=tweets_df.loc[has_gps].index)\n",
    "    tweets_df = tweets_df.join(places_geodf, on='place_id', how='left')\n",
    "    coords_in_place = tweets_coords.within(geopd.GeoSeries(tweets_df.loc[has_gps, 'geometry']))\n",
    "    \n",
    "    tweeted_months = tweets_counts.increment_counts(\n",
    "        tweeted_months, tweets_df, ['month'])\n",
    "    tweets_pb_months = tweets_counts.increment_counts(\n",
    "        tweets_pb_months, tweets_df.loc[has_gps].loc[~coords_in_place], ['month'])\n",
    "#     print(coords_in_place.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "months_counts = tweeted_months.join(tweets_pb_months, rsuffix='_pb', how='left')\n",
    "months_counts['prop'] = months_counts['count_pb'] / months_counts['count']\n",
    "ax = months_counts['prop'].plot.bar()\n",
    "ticks = np.arange(0,47,5)\n",
    "tick_labels = ax.get_xticklabels()\n",
    "_ = ax.set_xticks(ticks)\n",
    "_ = ax.set_xticklabels([tick_labels[i] for i in ticks])\n",
    "_ = ax.set_ylabel('proportion')\n",
    "_ = ax.set_title('Proportion of tweets with coords outside of place')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Filtering out users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Filters: user-based imply a loop over all the raw_tweets_df, and must be applied before getting tweets_lang_df and even tweets_loc_df, because these don't interest us at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is incremental, so can't parallelize. And it's rather fast, so not worth the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweeted_months_users = None\n",
    "for res in tweets_access_res:\n",
    "    raw_tweets_df = res.get()\n",
    "    nr_users = len(raw_tweets_df['uid'].unique())\n",
    "    print(f'There are {nr_users} distinct users in this chunk.')\n",
    "    tweeted_months_users = ufilters.inc_months_activity(\n",
    "        tweeted_months_users, raw_tweets_df)\n",
    "\n",
    "tweeted_months_users = tweeted_months_users['count']\n",
    "total_nr_users = len(tweeted_months_users.index.levels[0])\n",
    "print(f'In total, there are {total_nr_users} distinct users in the whole dataset.')\n",
    "local_uids = ufilters.consec_months(tweeted_months_users)\n",
    "bot_uids = ufilters.bot_activity(tweeted_months_users)\n",
    "# We have local_uids: index of uids with a column full of True, and bot_uids:\n",
    "# index of uids with a column full of False. When we multiply them, the uids\n",
    "# in local_uids which are not in bot_uids are assigned NaN, and the ones which \n",
    "# are in bot_uids are assigned False. When we convert to the boolean type,\n",
    "# the NaNs turn to True.\n",
    "valid_uids = (local_uids * bot_uids).astype('bool').rename('valid')\n",
    "valid_uids = valid_uids.loc[valid_uids]\n",
    "print(f'This leaves us with {len(valid_uids)} valid users in the whole dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def speed_filter(raw_tweets_df, valid_uids, places_in_xy, max_distance):\n",
    "    tweets_df = raw_tweets_df.join(valid_uids, on='uid', how='inner')\n",
    "    too_fast_uids = ufilters.too_fast(tweets_df, places_in_xy, max_distance)\n",
    "    return too_fast_uids\n",
    "\n",
    "too_fast_uids_series = pd.Series([])\n",
    "area_bounds = shape_df.to_crs(xy_proj).geometry.iloc[0].bounds\n",
    "# Get an upper limit of the distance that can be travelled inside the area\n",
    "max_distance = np.sqrt((area_bounds[0]-area_bounds[2])**2 \n",
    "                       + (area_bounds[1]-area_bounds[3])**2)\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    cols = ['uid', 'created_at', 'place_id', 'coordinates']\n",
    "    map_parameters = [\n",
    "        (res.get().loc[:, cols], valid_uids, places_in_xy, max_distance) \n",
    "        for res in tweets_access_res]\n",
    "    print('entering the loop')\n",
    "    too_fast_uids_list = pool.starmap_async(speed_filter, map_parameters).get()\n",
    "    for too_fast_uids in too_fast_uids_list:\n",
    "        too_fast_uids_series = (too_fast_uids_series * too_fast_uids).fillna(False)\n",
    "\n",
    "print(f'In total, there are {len(too_fast_uids_series)} too fast users left to '\n",
    "      'filter out in the whole dataset.')\n",
    "valid_uids = (valid_uids * too_fast_uids_series).astype('bool').rename('valid')\n",
    "valid_uids = valid_uids.loc[valid_uids]\n",
    "print(f'This leaves us with {len(valid_uids)} valid users in the whole dataset.')\n",
    "valid_uids.index = valid_uids.index.rename('uid')\n",
    "valid_uids.to_csv(valid_uids_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "most tweets in the month in that country to asign local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We don't filter out tweets with a useless place (one too large) here, because these tweets can still be useful for language detection. So this filter is only applied later on. Similarly, we keep tweets with insufficient text to make a reliable language detection, because they can still be useful for residence attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_uids = pd.read_csv(valid_uids_path, index_col='uid', header=0)\n",
    "\n",
    "def process(raw_tweets_df, valid_uids, places_geodf, langs_agg_dict, \n",
    "            text_col='text', min_nr_words=4, cld='pycld2'):\n",
    "    cols = ['text', 'id', 'lang', 'place_id', 'coordinates', 'uid', \n",
    "            'created_at', 'source']\n",
    "    tweets_loc_df = raw_tweets_df.loc[:, cols]\n",
    "    print('- starting geo join')\n",
    "    tweets_loc_df = tweets_loc_df.join(valid_uids, on='uid', how='inner')\n",
    "    # Happened for Quebec to get empty dataframes, to avoid errors we return\n",
    "    # here and get rid of those later on.\n",
    "    if tweets_loc_df.shape[0] == 0:\n",
    "        return None\n",
    "    has_gps = tweets_loc_df['coordinates'].notnull()\n",
    "    tweets_places_df = tweets_loc_df.loc[~has_gps].join(\n",
    "        places_geodf[['geometry', 'area']], on='place_id', how='left')\n",
    "    # The geometry of the tweets with GPS coordinates is the Point associated \n",
    "    # to them.\n",
    "    tweets_loc_df.loc[has_gps, 'geometry'] = (\n",
    "        tweets_loc_df.loc[has_gps, 'coordinates']\n",
    "                     .apply(lambda x: Point(x['coordinates'])))\n",
    "    tweets_loc_df = geopd.GeoDataFrame(tweets_loc_df, crs=latlon_proj)\n",
    "    tweets_loc_df.loc[has_gps, 'geometry'] = (\n",
    "        tweets_loc_df.loc[has_gps, 'geometry'].to_crs(places_geodf.crs))\n",
    "    # Since the projection was done on part of the GeoSeries, the crs parameter\n",
    "    # doesn't get changed, so we do it manually.\n",
    "    tweets_loc_df.crs = places_geodf.crs\n",
    "    # We assign the area of points to 0, and at the same time initialize the \n",
    "    # whole column, whose values will change for tweets without GPS coordinates.\n",
    "    tweets_loc_df['area'] = 0\n",
    "    # We add the geometry of the place to the tweets without GPS coordinates\n",
    "    tweets_loc_df.loc[~has_gps, 'geometry'] = tweets_places_df['geometry']\n",
    "    tweets_loc_df.loc[~has_gps, 'area'] = tweets_places_df['area']\n",
    "    tweets_loc_df = (tweets_loc_df.rename(columns={'lang': 'twitter_lang'})\n",
    "                                  .drop(columns=['valid', 'coordinates']))\n",
    "    print('starting lang detect')\n",
    "    tweets_lang_df = text_process.lang_detect(tweets_loc_df, text_col=text_col, \n",
    "        min_nr_words=min_nr_words, cld=cld, langs_agg_dict=langs_agg_dict)\n",
    "    print('chunk done')\n",
    "    return tweets_lang_df\n",
    "\n",
    "\n",
    "def profile_process(raw_tweets_df, valid_uids, places_geodf):\n",
    "    cProfile.runctx(\n",
    "        'process(raw_tweets_df, valid_uids, places_geodf)', globals(), locals())\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    map_parameters = [(res.get(), valid_uids, places_geodf, langs_agg_dict) \n",
    "                      for res in tweets_access_res]\n",
    "    print('entering the loop')\n",
    "    tweets_process_res = pool.starmap_async(process, map_parameters).get()\n",
    "    \n",
    "# We get rid of the null results    \n",
    "tweets_process_res = [res for res in tweets_process_res if res is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Study at the user level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Users who have tagged their tweets with gps coordinates seem to do it regularly, as the median of the proportion of tweets they geo tag is at more than 75% on the first chunk -> it's worth it to try and get their cell of residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = tweets_process_res[0].copy()\n",
    "a['has_gps'] = a['area'] == 0\n",
    "gps_uids = a.loc[a['has_gps'], 'uid'].unique()\n",
    "a = a.loc[a['uid'].isin(gps_uids)].groupby(['uid', 'has_gps']).size().rename('count').to_frame()\n",
    "a = a.join(a.groupby('uid')['count'].sum().rename('sum'))\n",
    "b = a.reset_index()\n",
    "b = b.loc[b['has_gps']]\n",
    "b['ratio'] = b['count'] / b['sum']\n",
    "b['ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If there's one or more cells where a user tweeted in proportion more than relevant_th of the time, we take among these cells the one where they tweeted the most outside work hours. Otherwise, we take the relevant place where they tweeted the most outside work hours, or we default to the place where they tweeted the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "user_level_label = '{}-speaking users'\n",
    "relevant_th = 0.1\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Language(s) attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we get rid of users whose language we couldn't identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Residence attribution is the longest to run, and by a long shot, so we'll start\n",
    "# with language to filter out uids in tweets_df before doing it\n",
    "groupby_cols = ['uid', 'cld_lang']\n",
    "user_langs_counts = None\n",
    "for res in tweets_process_res:\n",
    "    tweets_lang_df = res.copy()\n",
    "    # Here we don't filter out based on max_place_area, because these tweets\n",
    "    # are still useful for language attribution.\n",
    "    tweets_lang_df = tweets_lang_df.loc[tweets_lang_df['cld_lang'].notnull()]\n",
    "    user_langs_counts = join_and_count.increment_counts(\n",
    "        user_langs_counts, tweets_lang_df, groupby_cols)\n",
    "    \n",
    "total_per_user = user_langs_counts.groupby('uid')['count'].sum().rename('user_count')\n",
    "user_langs_agg = user_langs_counts.join(total_per_user).assign(\n",
    "    prop_lang=lambda df: df['count'] / df['user_count'])\n",
    "user_langs_agg = user_langs_agg.loc[user_langs_agg['prop_lang'] > relevant_th]\n",
    "uid_with_lang = user_langs_agg.index.levels[0].values\n",
    "print(f'We were able to attribute at least one language to {len(uid_with_lang)}'\n",
    "      ' users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Pre-residence attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prep_resid_attr(tweets_lang_df, cells_in_area_df, uid_with_lang, \n",
    "                    max_place_area, cc_timezone):\n",
    "    tweets_df = tweets_lang_df.copy()\n",
    "    # We filter out users to which we couldn't attribute even one language\n",
    "    uid_mask = tweets_df['uid'].isin(uid_with_lang)\n",
    "    relevant_area_mask = tweets_df['area'] < max_place_area\n",
    "    tweets_df = tweets_df.loc[uid_mask & relevant_area_mask].copy()\n",
    "    tweets_df['hour'] = (tweets_df['created_at'].dt.tz_localize('UTC')\n",
    "                                                .dt.tz_convert(cc_timezone)\n",
    "                                                .dt.hour)\n",
    "    # Tweets are considered in work hours if they were made between 8 and 18\n",
    "    # outside of the week-end (weekday goes from 0 (Monday) to 6 (Sunday)).\n",
    "    tweets_df['isin_workhour'] = (\n",
    "        (tweets_df['hour'] > 7) \n",
    "        & (tweets_df['hour'] < 18)\n",
    "        & (tweets_df['created_at'].dt.weekday < 5))\n",
    "    \n",
    "    has_gps = tweets_df['area'] == 0\n",
    "    tweets_cells_df = geopd.sjoin(tweets_df.loc[has_gps], cells_in_area_df, \n",
    "        op='within', rsuffix='cell', how='inner')\n",
    "    tweets_places_df = tweets_df.loc[~has_gps]\n",
    "    print('chunk done')\n",
    "    return tweets_cells_df, tweets_places_df\n",
    "\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    map_parameters = [(res, cells_in_area_df, uid_with_lang, max_place_area, \n",
    "                       cc_timezone) \n",
    "                      for res in tweets_process_res]\n",
    "    print('entering the loop')\n",
    "    tweets_pre_resid_res = (\n",
    "        pool.starmap_async(prep_resid_attr, map_parameters).get())\n",
    "    \n",
    "user_places_habits = None\n",
    "user_cells_habits = None\n",
    "for res in tweets_pre_resid_res:\n",
    "    # We first count the number of times a user has tweeted in each place inside\n",
    "    # and outside work hours.\n",
    "    tweets_places_df = res[1]\n",
    "    groupby_cols = ['uid', 'place_id', 'isin_workhour']\n",
    "    user_places_habits = join_and_count.increment_counts(\n",
    "        user_places_habits, tweets_places_df, groupby_cols)\n",
    "    # Then we do the same thing except in each cell, using the tweets with\n",
    "    # coordinates.\n",
    "    tweets_cells_df = res[0]\n",
    "    groupby_cols = ['uid', 'cell_id', 'isin_workhour']\n",
    "    user_cells_habits = join_and_count.increment_counts(\n",
    "        user_cells_habits, tweets_cells_df, groupby_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we took number of speakers, whether they're multilingual or monolingual, if they speak a language, they count as one in that language's count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Other possibility: pass the places counts to cells counts here, and then do the whole residence attribution solely\n",
    "on a cell basis. Problem: user tags himself in the same city all the time, overlapping multiple cells: we'll have more than one cell with approximately the same count, and one cells takes all in the end. Typical 'winner takes all' problem in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Residence attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We calculate the total number of users in each cell and place of residence.\n",
    "user_counts = user_places_habits.groupby('uid')['count'].sum().rename('user_count')\n",
    "user_home_cell = user_cells_habits.join(user_counts, how='inner')\n",
    "user_home_cell['prop_in_cell'] = user_home_cell['count'] / user_home_cell['user_count']\n",
    "user_home_cell = (user_home_cell.loc[user_home_cell['prop_in_cell'] > relevant_th]\n",
    "                                .xs(False, level='isin_workhour')\n",
    "                                .reset_index()\n",
    "                                .sort_values(by=['uid', 'count'])\n",
    "                                .groupby('uid')['cell_id']\n",
    "                                .last())\n",
    "\n",
    "users_with_cell = user_home_cell.index.values\n",
    "user_home_place = uagg.get_residence(user_places_habits, place_id_col='place_id')\n",
    "user_only_place = user_home_place.drop(users_with_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Generate cell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We get all the places with residents and the associated count\n",
    "places_counts = (user_only_place.to_frame()\n",
    "                                .groupby('place_id')\n",
    "                                .size()\n",
    "                                .rename('total_count')\n",
    "                                .to_frame())\n",
    "cells_counts = (user_home_cell.to_frame()\n",
    "                              .groupby('cell_id')\n",
    "                              .size()\n",
    "                              .rename('total_count')\n",
    "                              .to_frame())\n",
    "# We count the number of users speaking a local language in each cell and place \n",
    "# of residence.\n",
    "local_lang_users = user_langs_agg.reset_index(level='cld_lang')\n",
    "local_langs = [lang for lang in plot_langs_dict]\n",
    "local_langs_mask = local_lang_users['cld_lang'].isin(local_langs)\n",
    "local_lang_users = (local_lang_users.loc[local_langs_mask]\n",
    "                                    .groupby('uid')\n",
    "                                    .first())\n",
    "places_local_counts = uagg.to_count_by_area(local_lang_users, user_only_place,\n",
    "                                            output_col='local_count')\n",
    "cells_local_counts = uagg.to_count_by_area(local_lang_users, user_home_cell, \n",
    "                                           output_col='local_count')\n",
    "# Then we get the counts of speakers by language and cell\n",
    "places_langs_counts = uagg.to_count_by_area(user_langs_agg, user_only_place)\n",
    "cells_langs_counts = uagg.to_count_by_area(user_langs_agg, user_home_cell)\n",
    "# Then the counts of groups (mono-, bi-, tri-linguals):\n",
    "places_ling_counts = uagg.to_count_by_area(users_ling_grp, user_only_place)\n",
    "cells_ling_counts = uagg.to_count_by_area(users_ling_grp, user_home_cell)\n",
    "\n",
    "# We always left join on places counts, because total_count == 0 implies\n",
    "# that every other count is 0.\n",
    "places_counts = places_counts.join(places_local_counts, how='left')\n",
    "cells_counts = cells_counts.join(cells_local_counts, how='left')\n",
    "# TODO:TODO regroup in one for loop\n",
    "for ling in multiling_grps:\n",
    "    ling_count_col= f'count_{ling}'\n",
    "    cells_in_that_grp_count = (cells_ling_counts.xs(ling, level='ling_grp')\n",
    "                                                .rename(ling_count_col))\n",
    "    places_in_that_grp_count = (places_ling_counts.xs(ling, level='ling_grp')\n",
    "                                                  .rename(ling_count_col))\n",
    "    cells_counts = cells_counts.join(cells_in_that_grp_count, how='left')\n",
    "    places_counts = places_counts.join(places_in_that_grp_count, how='left')\n",
    "    \n",
    "for plot_lang, lang_dict in plot_langs_dict.items():\n",
    "    lang_count_col = lang_dict['count_col']\n",
    "    places_lang_counts = (places_langs_counts.xs(plot_lang, level='cld_lang')\n",
    "                                             .rename(lang_count_col))\n",
    "    cells_lang_counts = (cells_langs_counts.xs(plot_lang, level='cld_lang')\n",
    "                                           .rename(lang_count_col))\n",
    "    places_counts = places_counts.join(places_lang_counts, how='left')\n",
    "    cells_counts = cells_counts.join(cells_lang_counts, how='left')\n",
    "\n",
    "cell_plot_df = cells_in_area_df.copy()\n",
    "cells_in_places = places_to_cells.get_intersect(cell_plot_df, places_geodf, \n",
    "                                                places_counts, xy_proj=xy_proj)\n",
    "\n",
    "# Then we add the total counts for the users with a cell of residence.\n",
    "count_cols = places_counts.columns\n",
    "cell_plot_df = places_to_cells.intersect_to_cells(\n",
    "    cells_in_places, cell_plot_df, count_cols)\n",
    "for col in count_cols:\n",
    "    cell_plot_df = join_and_count.increment_join(\n",
    "        cell_plot_df, cells_counts[col], count_col=col)\n",
    "# 0 or 1?\n",
    "cell_plot_df = cell_plot_df.loc[cell_plot_df['total_count'] > 0]\n",
    "\n",
    "for plot_lang, lang_dict in plot_langs_dict.items():\n",
    "    lang_count_col = lang_dict['count_col']\n",
    "    level_lang_label = user_level_label.format(lang_dict['readable'])\n",
    "    sum_lang = cell_plot_df[lang_count_col].sum()\n",
    "    print(f'There are {sum_lang:.0f} {level_lang_label}.')\n",
    "\n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df.to_file(cell_data_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cell_size = 5000\n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(cell_plot_df, \n",
    "                                                     plot_langs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for plot_lang, plot_dict in plot_langs_dict.items():\n",
    "    count_lang_col = plot_dict['count_col']\n",
    "    readable_lang = plot_dict['readable']\n",
    "    save_path = os.path.join(fig_dir, 'counts',\n",
    "        f'users_counts_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Distribution of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['count_label']\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='Purples')\n",
    "    ax_count = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=count_lang_col, save_path=save_path, \n",
    "        show=False, log_scale=True, title=plot_title, cbar_label=cbar_label,\n",
    "        xy_proj=xy_proj, **plot_kwargs)\n",
    "    \n",
    "    prop_lang_col = plot_dict['prop_col']\n",
    "    save_path = os.path.join(fig_dir, 'prop',\n",
    "        f'users_prop_cc={cc}_lang={plot_lang}_cell_size={cell_size}m.pdf')\n",
    "    plot_title = f'Predominance of {readable_lang} speakers in {country_name}'\n",
    "    cbar_label = plot_dict['prop_label']\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax_prop = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=prop_lang_col, save_path=save_path, \n",
    "        title=plot_title, cbar_label=cbar_label, vmin=0, vmax=1, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join(fig_dir, \n",
    "            f'users_prop_cc={cc}_cell_size={cell_size}m.html')\n",
    "prop_dict = {'name': 'prop', 'readable': 'proportion', 'vmin': 0, 'vmax': 1}\n",
    "fig = grid_viz.plot_interactive(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, prop_dict,\n",
    "    save_path=save_path, plotly_renderer='iframe_connected', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL_dict = {'name': 'KL', 'readable': 'KL divergence', 'max_fun': metrics.max_kl}\n",
    "H_dict = {'name': 'H', 'readable': 'concentration entropy', 'max_fun': metrics.max_H}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deal with null cells, how do they impact the metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'BE'\n",
    "region = None\n",
    "cell_size = 10000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_langs_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two kinds of null values: total count is 0 and lang count is 0\n",
    "\n",
    "atm, a null lang count is in grey, and a null total count in white. choose a visually more pleasant color for null lang count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = helpers_viz.repr_grid(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, country_name, cmap='coolwarm',\n",
    "    save_path=None, xy_proj=xy_proj, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_lang, plot_dict in plot_langs_dict.items():\n",
    "    readable_lang = plot_dict['readable']\n",
    "    plot_title = f'Concentration entropy of {readable_lang} speakers in {country_name}'\n",
    "    H_col = plot_dict['H_col']\n",
    "    cbar_label = 'entropy'\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax_prop = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=H_col, save_path=None, \n",
    "        title=plot_title, cbar_label=cbar_label, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_lang, plot_dict in plot_langs_dict.items():\n",
    "    readable_lang = plot_dict['readable']\n",
    "    plot_title = f'KL divergence of {readable_lang} speakers in {country_name}'\n",
    "    KL_col = plot_dict['KL_col']\n",
    "    cbar_label = 'entropy'\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax_prop = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, metric_col=KL_col, save_path=None, \n",
    "        title=plot_title, cbar_label=cbar_label, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grps_dict = plot_langs_dict\n",
    "H, max_H = metrics.all_grps_metric(H_dict, cell_plot_df, grps_dict)\n",
    "KL, max_KL = metrics.all_grps_metric(KL_dict, cell_plot_df, grps_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wouldnt log(repr) be better suited? Problem: cells with repr == 0 -> then clusters with entropy/KL div?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [lang['repr_col'] for lang in plot_langs_dict.values()]\n",
    "cells_vectors = cell_plot_df[cols].values\n",
    "_, all_cells_clusters, _, ax = metrics.clusters(cells_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_to_keep = 2\n",
    "cell_plot_df['cluster'] = all_cells_clusters[n_clusters_to_keep-1]\n",
    "plot_title = f'clusters in {country_name}'\n",
    "cbar_label = None\n",
    "plot_kwargs = dict(edgecolor='w', linewidths=0.2, \n",
    "                   cmap=cm.get_cmap('cividis', n_clusters_to_keep))\n",
    "ax = grid_viz.plot_grid(\n",
    "    cell_plot_df, shape_df, metric_col='cluster', save_path=None, \n",
    "    title=plot_title, cbar_label=cbar_label, xy_proj=xy_proj, \n",
    "    **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other cell size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'BE'\n",
    "region = None\n",
    "cell_size = 5000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_langs_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = helpers_viz.repr_grid(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, country_name, cmap='coolwarm',\n",
    "    save_path=None, xy_proj=xy_proj, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grps_dict = plot_langs_dict\n",
    "H, max_H = metrics.all_grps_metric(H_dict, cell_plot_df, grps_dict)\n",
    "KL, max_KL = metrics.all_grps_metric(KL_dict, cell_plot_df, grps_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cataluña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'ES'\n",
    "region = 'Cataluña'\n",
    "cell_size = 10000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_langs_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = helpers_viz.repr_grid(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, country_name, cmap='coolwarm',\n",
    "    save_path=None, xy_proj=xy_proj, min_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H are surpassing 1, norm problem here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grps_dict = plot_langs_dict\n",
    "H, max_H = metrics.all_grps_metric(H_dict, cell_plot_df, grps_dict)\n",
    "KL, max_KL = metrics.all_grps_metric(KL_dict, cell_plot_df, grps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [lang['repr_col'] for lang in plot_langs_dict.values()]\n",
    "cells_vectors = cell_plot_df[cols].values\n",
    "_, all_cells_clusters, _, ax = metrics.clusters(cells_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 2 clusters, we explain just a bit more than 50% of the variance -> can illustrate qualitative difference bewteen cat and be or ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_to_keep = 3\n",
    "cell_plot_df['cluster'] = all_cells_clusters[n_clusters_to_keep-1]\n",
    "plot_title = f'clusters in {country_name}'\n",
    "cbar_label = None\n",
    "# Avoid sequential colormaps starting or ending with white, as white is  \n",
    "# reserved for an absence of data\n",
    "plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap=cm.get_cmap('cividis', n_clusters_to_keep))\n",
    "ax = grid_viz.plot_grid(\n",
    "    cell_plot_df, shape_df, metric_col='cluster', save_path=None, \n",
    "    title=plot_title, cbar_label=cbar_label, xy_proj=xy_proj, \n",
    "    **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'CH'\n",
    "region = None\n",
    "cell_size = 10000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_langs_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = helpers_viz.repr_grid(\n",
    "    cell_plot_df, shape_df, plot_langs_dict, country_name, cmap='coolwarm',\n",
    "    save_path=None, xy_proj=xy_proj, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grps_dict = plot_langs_dict\n",
    "H, max_H = metrics.all_grps_metric(H_dict, cell_plot_df, grps_dict)\n",
    "KL, max_KL = metrics.all_grps_metric(KL_dict, cell_plot_df, grps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [lang['repr_col'] for lang in plot_langs_dict.values()]\n",
    "cells_vectors = cell_plot_df[cols].values\n",
    "_, all_cells_clusters, _, ax = metrics.clusters(cells_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_to_keep = 3\n",
    "cell_plot_df['cluster'] = all_cells_clusters[n_clusters_to_keep-1]\n",
    "plot_title = f'clusters in {country_name}'\n",
    "cbar_label = None\n",
    "# Avoid sequential colormaps starting or ending with white, as white is  \n",
    "# reserved for an absence of data\n",
    "plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap=cm.get_cmap('cividis', n_clusters_to_keep))\n",
    "ax = grid_viz.plot_grid(\n",
    "    cell_plot_df, shape_df, metric_col='cluster', save_path=None, \n",
    "    title=plot_title, cbar_label=cbar_label, xy_proj=xy_proj, \n",
    "    **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinguals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe fractal dimension (-> if close to 1, means biliguals on border, on a line)?  use more the proportions for ling groups, and try fabio like entropy that could work well here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'CH'\n",
    "region = None\n",
    "cell_size = 10000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "plot_lings_dict = make_config.linguals_dict(area_dict)\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_lings_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_lings_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_plot_df['H_prop'] = 0\n",
    "for plot_ling, plot_dict in plot_lings_dict.items():\n",
    "    count_ling_col = plot_dict['count_col']\n",
    "    ling_label = plot_dict['grp_label']\n",
    "    prop_ling_col = plot_dict['prop_col']\n",
    "    cell_plot_df['H_prop'] += cell_plot_df.apply(\n",
    "        lambda df: metrics.entropy(df[count_ling_col] / df['total_count']), axis=1)\n",
    "    \n",
    "cell_plot_df['H_prop'] = cell_plot_df['H_prop'] / np.log(len(plot_lings_dict))\n",
    "plot_title = f'Cell proportion entropy of mono/multi-lingual groups in {country_name}'\n",
    "cbar_label = plot_dict['prop_label']\n",
    "# Avoid sequential colormaps starting or ending with white, as white is  \n",
    "# reserved for an absence of data\n",
    "plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "ax = grid_viz.plot_grid(\n",
    "    cell_plot_df, shape_df, \n",
    "    metric_col='H_prop', save_path=None, vmin=0, vmax=1,\n",
    "    title=plot_title, cbar_label=cbar_label, xy_proj=xy_proj, \n",
    "    **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_plot_df['H_prop'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = helpers_viz.repr_grid(\n",
    "    cell_plot_df, shape_df, plot_lings_dict, country_name, cmap='coolwarm',\n",
    "    save_path=None, xy_proj=xy_proj, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'CH'\n",
    "region = None\n",
    "cell_size = 5000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "plot_lings_dict = make_config.linguals_dict(area_dict)\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_lings_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_lings_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = helpers_viz.repr_grid(\n",
    "    cell_plot_df, shape_df, plot_lings_dict, country_name, cmap='coolwarm',\n",
    "    save_path=None, xy_proj=xy_proj, min_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'BE'\n",
    "region = None\n",
    "cell_size = 10000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "plot_lings_dict = make_config.linguals_dict(area_dict)\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_lings_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_lings_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = helpers_viz.repr_grid(\n",
    "    cell_plot_df, shape_df, plot_lings_dict, country_name, cmap='coolwarm',\n",
    "    save_path=None, xy_proj=xy_proj, min_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cataluña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'ES'\n",
    "region = 'Cataluña'\n",
    "cell_size = 10000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "plot_lings_dict = make_config.linguals_dict(area_dict)\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_lings_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_lings_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = helpers_viz.repr_grid(\n",
    "    cell_plot_df, shape_df, plot_lings_dict, country_name, cmap='coolwarm',\n",
    "    save_path=None, xy_proj=xy_proj, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_ling, plot_dict in plot_lings_dict.items():\n",
    "    count_ling_col = plot_dict['count_col']\n",
    "    ling_label = plot_dict['grp_label']\n",
    "    prop_ling_col = plot_dict['prop_col']\n",
    "    plot_title = f'Predominance of {ling_label} in {country_name}'\n",
    "    cbar_label = plot_dict['prop_label']\n",
    "    # Avoid sequential colormaps starting or ending with white, as white is  \n",
    "    # reserved for an absence of data\n",
    "    plot_kwargs = dict(edgecolor='w', linewidths=0.2, cmap='plasma')\n",
    "    ax = grid_viz.plot_grid(\n",
    "        cell_plot_df, shape_df, \n",
    "        metric_col=prop_ling_col, save_path=None, vmin=0, vmax=1,\n",
    "        title=plot_title, cbar_label=cbar_label, xy_proj=xy_proj, \n",
    "        **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peacock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'CH'\n",
    "region = None\n",
    "# region = 'Cataluña'\n",
    "cell_size = 10000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_langs_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peacock's KS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, Nx, Ny = geo.create_grid(\n",
    "    shape_df, cell_size, xy_proj=xy_proj, intersect=False)\n",
    "\n",
    "for lang, lang_dict in plot_langs_dict.items():\n",
    "    count_col = lang_dict['count_col']\n",
    "    conc_col = lang_dict['conc_col']\n",
    "    n_samples = cell_plot_df[count_col].sum()\n",
    "    ks_score, p_value = metrics.ks_test_2d(cell_plot_df, conc_col, 'total_conc', \n",
    "                                           Nx, Ny, n_samples)\n",
    "    print(lang, ks_score, p_value)\n",
    "    chi2_score, normed_score, p_value = metrics.grid_chisquare(\n",
    "        cell_plot_df, conc_col, 'total_conc', n_samples)\n",
    "    print(lang, normed_score, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'BE'\n",
    "region = None\n",
    "# region = 'Cataluña'\n",
    "cell_size = 10000\n",
    "user_level_label = '{}-speaking users'\n",
    "\n",
    "area_dict = make_config.area_dict(countries_study_data, cc, region=region)\n",
    "xy_proj = area_dict['xy_proj']\n",
    "plot_langs_dict = make_config.langs_dict(area_dict, user_level_label)\n",
    "shapefile_dict = make_config.shapefile_dict(area_dict, cc, region=region)\n",
    "    \n",
    "shapefile_path = os.path.join(\n",
    "    external_data_dir, shapefile_dict['name'], shapefile_dict['name'])\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = geo.extract_shape(shape_df, shapefile_dict['col'], \n",
    "                             shapefile_dict['val'], xy_proj=xy_proj)\n",
    "\n",
    "if region:\n",
    "    country_name = region\n",
    "else:\n",
    "    country_name = shape_df['NAME_ENGL'].iloc[0]\n",
    "    \n",
    "cell_data_path = cell_data_path_format.format('users', cc, cell_size)\n",
    "cell_plot_df = geopd.read_file(cell_data_path)\n",
    "cell_plot_df.index = cell_plot_df['cell_id']\n",
    "cell_plot_df, plot_langs_dict = metrics.calc_by_cell(\n",
    "    cell_plot_df, plot_langs_dict)\n",
    "cell_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, Nx, Ny = geo.create_grid(\n",
    "    shape_df, cell_size, xy_proj=xy_proj, intersect=False)\n",
    "\n",
    "for lang, lang_dict in plot_langs_dict.items():\n",
    "    count_col = lang_dict['count_col']\n",
    "    conc_col = lang_dict['conc_col']\n",
    "    n_samples = cell_plot_df[count_col].sum()\n",
    "    ks_score, p_value = metrics.ks_test_2d(cell_plot_df, conc_col, 'total_conc', \n",
    "                                           Nx, Ny, n_samples)\n",
    "    print(lang, ks_score, p_value)\n",
    "    chi2_score, normed_score, p_value = metrics.grid_chisquare(\n",
    "        cell_plot_df, conc_col, 'total_conc', n_samples)\n",
    "    print(lang, normed_score, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "272px",
    "width": "238px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 220.85,
   "position": {
    "height": "242.85px",
    "left": "631px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
