{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all src modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cProfile\n",
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import gzip\n",
    "try:\n",
    "    import cld3\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "import pycld2\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import folium\n",
    "import src.utils.geometry as geo\n",
    "import src.data.shp_extract as shp_extract\n",
    "import src.data.tweets_cells_counts as tweets_counts\n",
    "import src.data.text_process as text_process\n",
    "import src.data.access as data_access\n",
    "import src.visualization.grid_viz as grid_viz\n",
    "import src.data.user_filters as ufilters\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too small 'places' data: BO, TN\n",
    "\n",
    "Limited 'places' data: LT: 69 and EE: 252 (only large cities), HK: 21 (only districts), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mixed distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = os.environ['DATA_DIR']\n",
    "tweets_files_format = 'tweets_2015_2018_{}.json.gz'\n",
    "places_files_format = 'places_2015_2018_{}.json.gz'\n",
    "ssh_domain = os.environ['IFISC_DOMAIN']\n",
    "ssh_username = os.environ['IFISC_USERNAME']\n",
    "country_codes = ('BO', 'CA', 'CH', 'EE', 'ES', 'FR', 'HK','ID', 'LT', 'LV',\n",
    "                'MY', 'PE', 'RO', 'SG', 'TN', 'UA')\n",
    "latlon_proj = 'epsg:4326'\n",
    "xy_proj = 'epsg:3857'\n",
    "external_data_dir = '../data/external/'\n",
    "fig_dir = '../reports/figures'\n",
    "cc = 'CH'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_path = os.path.join(data_dir_path, tweets_files_format.format(cc))\n",
    "chunk_size = 100000\n",
    "raw_tweets_df_generator = data_access.yield_json(tweets_file_path, \n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, chunk_size=chunk_size, compression='gzip')\n",
    "for i,raw_tweets_df in enumerate(raw_tweets_df_generator):\n",
    "    break\n",
    "\n",
    "raw_tweets_df_generator.close()\n",
    "ratio_coords = len(raw_tweets_df.loc[raw_tweets_df['coordinates'].notnull()]) / chunk_size\n",
    "print('{:.1%} of tweets have exact coordinates data'.format(ratio_coords))\n",
    "nr_users = len(raw_tweets_df['uid'].unique())\n",
    "print('There are {} distinct users in the dataset'.format(nr_users))\n",
    "raw_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_file_path = os.path.join(data_dir_path, places_files_format.format(cc))\n",
    "shapefile_name = 'CNTR_RG_01M_2016_4326.shp'\n",
    "shapefile_path = os.path.join(external_data_dir, shapefile_name, shapefile_name)\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = shape_df.loc[shape_df['FID'] == cc]\n",
    "raw_places_df = data_access.return_json(places_file_path, \n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, compression='gzip')\n",
    "raw_places_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get most frequent, small enough place: if most frequent -> select it, if within more frequent bigger place -> select it, \n",
    "\n",
    "If not small enough place, discard the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_tweets_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"I'm at \\<place\\>\" from Foursquare are also there, and they all have 'source' = <a href=\"http://foursquare.com\" rel=\"nofollow\">Foursquare</a>. Tweetbot is an app for regular users, it's not related to bot users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = raw_tweets_df[['text', 'id', 'lang', 'place_id', 'coordinates', 'uid', 'created_at']]\n",
    "tweets_df = tweets_df.rename(columns={'lang': 'twitter_lang'})\n",
    "null_reply_id = 'e39d05b72f25767869d44391919434896bb055772d7969f74472032b03bc18418911f3b0e6dd47ff8f3b2323728225286c3cb36914d28dc7db40bdd786159c0a'\n",
    "raw_tweets_df.loc[raw_tweets_df['in_reply_to_status_id'] == null_reply_id, \n",
    "    ['in_reply_to_status_id', 'in_reply_to_screen_name', 'in_reply_to_user_id']] = None\n",
    "tweets_df['source'] = raw_tweets_df['source'].str.extract(r'>(.+)</a>', expand=False)\n",
    "tweets_df['source'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = raw_tweets_df[raw_tweets_df['source'].str.contains('tweetmyjobs')]\n",
    "a = (a.drop(columns=['in_reply_to_status_id', 'id', 'source',  \n",
    "                'in_reply_to_screen_name', 'in_reply_to_user_id', 'quoted_status_id'])\n",
    "    .sort_values(by=['uid', 'created_at']))\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "a[a['uid'] == '066669353196d994d624138aa1ef4aafd892ed8e1e6e65532a39ecc7e6129b829bdbf8ea2b53b11f93a74cb7d1a3e1aa537d0c060be02778b37550d70a77a80d']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First tests on single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_year = 2015\n",
    "nr_consec_months = 3\n",
    "tweets_file_path = os.path.join(data_dir_path, tweets_files_format.format(cc))\n",
    "raw_tweets_df_generator = data_access.yield_json(tweets_file_path, \n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, chunk_size=1000000, compression='gzip')\n",
    "agg_tweeted_months_users = pd.DataFrame([], columns=['uid', 'month', 'count'])\n",
    "tweets_df_list = []\n",
    "for raw_tweets_df in raw_tweets_df_generator:\n",
    "    tweets_df_list.append(raw_tweets_df)\n",
    "    agg_tweeted_months_users = ufilters.inc_months_activity(\n",
    "            agg_tweeted_months_users, raw_tweets_df)\n",
    "raw_tweets_df_generator.close()\n",
    "local_uid_series = ufilters.consec_months(agg_tweeted_months_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_year = 2015\n",
    "nr_consec_months = 3\n",
    "tweeted_months_users = pd.DataFrame([], columns=['uid', 'month', 'count'])\n",
    "tweeted_months_users = ufilters.inc_months_activity(\n",
    "            tweeted_months_users, tweets_df)\n",
    "local_uid_series = ufilters.consec_months(tweeted_months_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets_df['lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets_df.join(local_uid_series, on='uid', how='inner')['lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_path = os.path.join(data_dir_path, tweets_files_format.format(cc))\n",
    "raw_tweets_df_generator = data_access.yield_json(tweets_file_path, \n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, chunk_size=1000000, compression='gzip')\n",
    "for raw_tweets_df in raw_tweets_df_generator:\n",
    "    filtered_tweets_df = pd.DataFrame(local_uid_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detected languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Languages possibly detected by CLD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_with_code = dict(pycld2.LANGUAGES)\n",
    "detected_lang_with_code = [(lang, lang_with_code[lang]) for lang in pycld2.DETECTED_LANGUAGES]\n",
    "print(detected_lang_with_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Languages possibly detected by Twitter (see 'lang' in https://support.gnip.com/apis/powertrack2.0/rules.html#Operators):\n",
    "\n",
    "Amharic - am\n",
    "Arabic - ar\n",
    "Armenian - hy\n",
    "Bengali - bn\n",
    "Bulgarian - bg\n",
    "Burmese - my\n",
    "Chinese - zh\n",
    "Czech - cs\n",
    "Danish - da\n",
    "Dutch - nl\n",
    "English - en\n",
    "Estonian - et\n",
    "Finnish - fi\n",
    "French - fr\n",
    "Georgian - ka\n",
    "German - de\n",
    "Greek - el\n",
    "Gujarati - gu\n",
    "Haitian - ht\n",
    "Hebrew - iw\n",
    "Hindi - hi\n",
    "Hungarian - hu\n",
    "Icelandic - is\n",
    "Indonesian - in\n",
    "Italian - it\n",
    "Japanese - ja\n",
    "Kannada - kn\n",
    "Khmer - km\n",
    "Korean - ko\n",
    "Lao - lo\n",
    "Latvian - lv\n",
    "Lithuanian - lt\n",
    "Malayalam - ml\n",
    "Maldivian - dv\n",
    "Marathi - mr\n",
    "Nepali - ne\n",
    "Norwegian - no\n",
    "Oriya - or\n",
    "Panjabi - pa\n",
    "Pashto - ps\n",
    "Persian - fa\n",
    "Polish - pl\n",
    "Portuguese - pt\n",
    "Romanian - ro\n",
    "Russian - ru\n",
    "Serbian - sr\n",
    "Sindhi - sd\n",
    "Sinhala - si\n",
    "Slovak - sk\n",
    "Slovenian - sl\n",
    "Sorani Kurdish - ckb\n",
    "Spanish - es\n",
    "Swedish - sv\n",
    "Tagalog - tl\n",
    "Tamil - ta\n",
    "Telugu - te\n",
    "Thai - th\n",
    "Tibetan - bo\n",
    "Turkish - tr\n",
    "Ukrainian - uk\n",
    "Urdu - ur\n",
    "Uyghur - ug\n",
    "Vietnamese - vi\n",
    "Welsh - cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lang_df = text_process.lang_detect(tweets_df, text_col='text', min_nr_words=4, cld='pycld2')\n",
    "tweets_lang_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld_langs = tweets_lang_df['cld_lang'].unique()\n",
    "cld_langs.sort()\n",
    "print('Languages detected by cld: {}'.format(cld_langs))\n",
    "twitter_langs = tweets_lang_df['twitter_lang'].unique()\n",
    "twitter_langs.sort()\n",
    "print('Languages detected by twitter: {}'.format(twitter_langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lang_df['twitter_lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lang_df['cld_lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French case, corsican is unreliably detected by CLD for French tweets, however seems pretty accurate when twitter_lang='it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_user_lang = tweets_lang_df.loc[tweets_lang_df['twitter_lang'] != 'und'].groupby(['uid', 'twitter_lang'])\n",
    "count_tweets_by_user_lang = groupby_user_lang.size()\n",
    "count_langs_by_user_df = count_tweets_by_user_lang.groupby('uid').transform('size')\n",
    "multiling_users_df = count_tweets_by_user_lang.loc[count_langs_by_user_df > 1]\n",
    "pd.DataFrame(multiling_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)\n",
    "multiling_users_list = [x[0] for x in multiling_users_df.index.values]\n",
    "tweets_lang_df[tweets_lang_df['uid'].isin(multiling_users_list)].sort_values(by=['uid', 'cld_lang'])[\n",
    "    ['uid', 'filtered_text', 'cld_lang', 'twitter_lang', 'created_at']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Places into geodf and join on tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the area to discard bbox which are too large? Problem: need to project first, which is expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_loc_df = tweets_lang_df.loc[tweets_lang_df['coordinates'].isnull()]\n",
    "crs = {'init': latlon_proj}\n",
    "places_df = raw_places_df[['id', 'bounding_box', 'name', 'place_type']]\n",
    "geometry = places_df['bounding_box'].apply(lambda x: Polygon(x['coordinates'][0]))\n",
    "places_geodf = geopd.GeoDataFrame(places_df, crs=crs, geometry=geometry)\n",
    "places_geodf = places_geodf.set_index('id')\n",
    "places_geodf = places_geodf.drop(columns=['bounding_box'])\n",
    "places_geodf['area'] = places_geodf.geometry.to_crs(xy_proj).area\n",
    "tweets_final_df = tweets_to_loc_df.join(places_geodf, on='place_id', how='left')\n",
    "tweets_final_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corsican?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final_df.loc[(tweets_final_df['cld_lang'] =='co') & (tweets_final_df['twitter_lang'] =='it')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLD sensitive to letter repetitions made to insist: can put threshold if more than 3 consecutive same letter, bring it down to 2, it seems to improve prediction on example\n",
    "\n",
    "Usually twitter's prediction seems better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final_df[tweets_final_df['cld_lang'] != tweets_final_df['twitter_lang']].drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swiss German?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zurich_id = places_geodf.loc[places_geodf['name']=='Zurich', 'geometry'].index[0]\n",
    "# places_in_zurich = places_geodf\n",
    "places_in_zurich = places_geodf.loc[places_geodf.within(places_geodf.loc[zurich_id, 'geometry'])]\n",
    "places_in_zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_in_zurich = tweets_final_df.join(places_in_zurich, on='place_id', rsuffix='_place')\n",
    "print(tweets_in_zurich['cld_lang'].value_counts().head())\n",
    "print(tweets_in_zurich['twitter_lang'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_in_zurich.loc[(tweets_in_zurich['cld_lang']=='un') & (tweets_in_zurich['twitter_lang']=='de'), \n",
    "                     'filtered_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly mixed languages not detected by twitter it seems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_in_zurich.loc[tweets_in_zurich['twitter_lang']=='und', \n",
    "                     'filtered_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupbys and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_time(df, dt_col):\n",
    "    t_series_in_sec_of_day = df['hour']*3600 + df['minute']*60 + df['second']\n",
    "    return pd.to_timedelta(int(t_series_in_sec_of_day.mean()), unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = raw_tweets_df.copy()\n",
    "# Speeds up the process to extract the hour, min and sec first\n",
    "tweets_df['hour'] = tweets_df['created_at'].dt.hour\n",
    "tweets_df['minute'] = tweets_df['created_at'].dt.minute\n",
    "tweets_df['second'] = tweets_df['created_at'].dt.second\n",
    "groupby_user_place = tweets_df.groupby(['uid', 'place_id'])\n",
    "count_tweets_by_user_place = groupby_user_place.size()\n",
    "count_tweets_by_user_place.rename('count', inplace=True)\n",
    "mean_time_by_user_place = groupby_user_place.apply(lambda df: get_mean_time(df, 'created_at'))\n",
    "mean_time_by_user_place.rename('avg time', inplace=True)\n",
    "# transform to keep same size, so as to be able to have a matching boolean Series of same size as \n",
    "# original df to select users with more than one place for example:\n",
    "count_places_by_user_df = count_tweets_by_user_place.groupby('uid').transform('size')\n",
    "agg_data_df = pd.concat([count_tweets_by_user_place, mean_time_by_user_place], axis=1)\n",
    "count_tweets_by_user_place_geodf = agg_data_df.join(places_geodf, on='place_id')\n",
    "count_tweets_by_user_place_geodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run(\"groupby_user_place.apply(lambda df: get_mean_time(df, 'created_at'))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweets_by_user_place_geodf.loc[count_places_by_user_df > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new chunk to cumulative data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweets_by_user_place_geodf = count_tweets_by_user_place_geodf.join(\n",
    "    count_tweets_by_user_place_geodf['count'], \n",
    "    on=['uid', 'place_id'], how='outer', rsuffix='_new')\n",
    "count_tweets_by_user_place_geodf['count'] += count_tweets_by_user_place_geodf['count_new']\n",
    "count_tweets_by_user_place_geodf.drop(columns=['count_new'], inplace=True)\n",
    "count_tweets_by_user_place_geodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
