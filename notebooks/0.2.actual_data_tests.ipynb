{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all src modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cProfile\n",
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import gzip\n",
    "try:\n",
    "    import cld3\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "import pycld2\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import folium\n",
    "import src.utils.geometry as geo\n",
    "import src.data.shp_extract as shp_extract\n",
    "import src.data.tweets_cells_counts as tweets_counts\n",
    "import src.data.text_process as text_process\n",
    "import src.data.access as data_access\n",
    "import src.visualization.grid_viz as grid_viz\n",
    "import src.data.user_filters as ufilters\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = os.environ['DATA_DIR']\n",
    "tweets_files_format = 'tweets_2015_2018_{}.json.gz'\n",
    "places_files_format = 'places_2015_2018_{}.json.gz'\n",
    "ssh_domain = os.environ['IFISC_DOMAIN']\n",
    "ssh_username = os.environ['IFISC_USERNAME']\n",
    "country_codes = ('BO', 'CA', 'CH', 'EE', 'ES', 'FR', 'HK','ID', 'LT', 'LV',\n",
    "                'MY', 'PE', 'RO', 'SG', 'TN', 'UA')\n",
    "latlon_proj = 'epsg:4326'\n",
    "xy_proj = 'epsg:3857'\n",
    "external_data_dir = '../data/external/'\n",
    "fig_dir = '../reports/figures'\n",
    "cc = 'CH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_path = os.path.join(data_dir_path, tweets_files_format.format(cc))\n",
    "raw_tweets_df_generator = data_access.yield_json(tweets_file_path, \n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, chunk_size=100000, compression='gzip')\n",
    "for i,raw_tweets_df in enumerate(raw_tweets_df_generator):\n",
    "    break\n",
    "\n",
    "raw_tweets_df_generator.close()\n",
    "raw_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_file_path = os.path.join(data_dir_path, places_files_format.format(cc))\n",
    "shapefile_name = 'CNTR_RG_01M_2016_4326.shp'\n",
    "shapefile_path = os.path.join(external_data_dir, shapefile_name, shapefile_name)\n",
    "shape_df = geopd.read_file(shapefile_path)\n",
    "shape_df = shape_df.loc[shape_df['FID'] == cc]\n",
    "raw_places_df = data_access.return_json(places_file_path, \n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, compression='gzip')\n",
    "raw_places_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what seems to be most countries, places include very few poi, for instance in CH, there are only two of them, in PE none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get most frequent, small enough place: if most frequent -> select it, if within more frequent bigger place -> select it, \n",
    "\n",
    "If not small enough place, discard the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_tweets_df.info())\n",
    "nr_users = len(raw_tweets_df['uid'].unique())\n",
    "print('There are {} distinct users in the dataset'.format(nr_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"I'm at \\<place\\>\" from Foursquare are also there, and they all have 'source' = <a href=\"http://foursquare.com\" rel=\"nofollow\">Foursquare</a>. Tweetbot is an app for regular users, it's not related to bot users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = raw_tweets_df[['text', 'id', 'lang', 'place_id', 'coordinates', 'uid', 'created_at']]\n",
    "tweets_df = tweets_df.rename(columns={'lang': 'twitter_lang'})\n",
    "null_reply_id = 'e39d05b72f25767869d44391919434896bb055772d7969f74472032b03bc18418911f3b0e6dd47ff8f3b2323728225286c3cb36914d28dc7db40bdd786159c0a'\n",
    "raw_tweets_df.loc[raw_tweets_df['in_reply_to_status_id'] == null_reply_id, \n",
    "    ['in_reply_to_status_id', 'in_reply_to_screen_name', 'in_reply_to_user_id']] = None\n",
    "tweets_df['source'] = raw_tweets_df['source'].str.extract(r'>(.+)</a>', expand=False)\n",
    "tweets_df['source'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get residents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_path = os.path.join(data_dir_path, tweets_files_format.format(cc))\n",
    "\n",
    "def pre_process(tweets_file_path, chunk_start, chunk_size):\n",
    "    raw_tweets_df = data_access.read_json_wrapper(tweets_file_path, chunk_start, chunk_size, \n",
    "                                                  ssh_domain=ssh_domain, ssh_username=ssh_username)\n",
    "    tweeted_months_users = ufilters.get_months_activity(raw_tweets_df)\n",
    "    return raw_tweets_df, tweeted_months_users\n",
    "\n",
    "def profile_pre_process(tweets_file_path, chunk_start, chunk_size):\n",
    "    cProfile.runctx('pre_process(tweets_file_path, chunk_start, chunk_size)', globals(), locals())\n",
    "\n",
    "agg_tweeted_months_users = pd.DataFrame([], columns=['uid', 'month'])\n",
    "with mp.Pool(4) as pool:\n",
    "    jobs = []\n",
    "    for chunk_start, chunk_size in data_access.chunkify(tweets_file_path, size=1e9, \n",
    "                                                        ssh_domain=ssh_domain, ssh_username=ssh_username):\n",
    "        jobs.append(pool.apply_async(pre_process, (tweets_file_path, chunk_start, chunk_size)))\n",
    "\n",
    "    for job in jobs:\n",
    "        raw_tweets_df, tweeted_months_users = job.get()\n",
    "        agg_tweeted_months_users = pd.concat([agg_tweeted_months_users, tweeted_months_users])\n",
    "total_nr_users = len(agg_tweeted_months_users['uid'].unique())\n",
    "print('In total, there are {} distinct users in the whole dataset'.format(total_nr_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_uid_series = ufilters.consec_months(agg_tweeted_months_users)\n",
    "crs = {'init': latlon_proj}\n",
    "places_df = raw_places_df[['id', 'bounding_box', 'name', 'place_type']]\n",
    "geometry = places_df['bounding_box'].apply(lambda x: Polygon(x['coordinates'][0]))\n",
    "places_geodf = geopd.GeoDataFrame(places_df, crs=crs, geometry=geometry)\n",
    "places_geodf = places_geodf.set_index('id')\n",
    "places_geodf = places_geodf.drop(columns=['bounding_box'])\n",
    "places_geodf['area'] = places_geodf.geometry.to_crs(xy_proj).area\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(raw_tweets_df, local_uid_series, places_geodf, text_col='text', min_nr_words=4, cld='pycld2'):\n",
    "    tweets_df = raw_tweets_df[['text', 'id', 'lang', 'place_id', 'coordinates', 'uid', 'created_at']]\n",
    "    tweets_df = tweets_df.loc[tweets_df['uid'].isin(local_uid_series.index.values)]\n",
    "    tweets_df = tweets_df.rename(columns={'lang': 'twitter_lang'})\n",
    "    print('starting lang detect')\n",
    "    tweets_lang_df = text_process.lang_detect(tweets_df, text_col='text', min_nr_words=4, cld='pycld2')\n",
    "    tweets_lang_df = tweets_lang_df.loc[tweets_lang_df['cld_lang'] != 'un']\n",
    "    tweets_to_loc_df = tweets_lang_df.loc[:]\n",
    "    tweets_final_df = tweets_to_loc_df.join(places_geodf, on='place_id', how='left')\n",
    "    print('processing done')\n",
    "    return tweets_final_df\n",
    "\n",
    "def profile_process(raw_tweets_df, local_uid_series, places_geodf):\n",
    "    cProfile.runctx('process(raw_tweets_df, local_uid_series, places_geodf)', globals(), locals())\n",
    "\n",
    "list_tweets_df = []\n",
    "with mp.Pool(8) as pool:\n",
    "    results = []\n",
    "    for job in jobs:\n",
    "        raw_tweets_df = job.get()[0]\n",
    "        results.append(pool.apply_async(process, (raw_tweets_df, local_uid_series, places_geodf)))\n",
    "\n",
    "    for res in results:\n",
    "        res.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = 5000\n",
    "cells_df, cells_in_area_df = geo.create_grid(shape_df, cell_size, latlon_proj, xy_proj, intersect=True)\n",
    "grid_test_df = cells_in_area_df.copy()\n",
    "grid_test_df['metric'] = 1\n",
    "save_path = os.path.join(fig_dir, 'grid_cc={}_cell_size={}m.pdf'.format(cc, cell_size))\n",
    "plot_kwargs = dict(alpha=0.7, edgecolor='w', linewidths=0.001)\n",
    "grid_viz.plot_grid(grid_test_df, shape_df, metric_col='metric', save_path=save_path, **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few users speaking Spanish but lots of tweets, must be a bot there\n",
    "\n",
    "pb: overlapping places' bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lang = 'es'\n",
    "max_place_area = 1e9\n",
    "agg_count_df = pd.Series(name='count', dtype='int64')\n",
    "for res in results:\n",
    "    df = res.get()\n",
    "    df = df.loc[(df['area'] < max_place_area) & (df['cld_lang'] == plot_lang)]\n",
    "    agg_count_df = tweets_counts.increment_counts(agg_count_df, df,\n",
    "                                                 'place_id')\n",
    "places_counts_df = places_geodf.join(agg_count_df, how='inner')\n",
    "print('There are {:n} tweets in {}.'.format(agg_count_df.sum(), plot_lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_in_area_df['cell_id'] = cells_in_area_df.index\n",
    "cells_intersect_places = geopd.overlay(places_counts_df, cells_in_area_df, how='intersection')\n",
    "cells_intersect_places['area_intersect'] = cells_intersect_places.geometry.to_crs(xy_proj).area\n",
    "cells_intersect_places['cell_count'] = cells_intersect_places['count'] * (cells_intersect_places['area_intersect']\n",
    "                                                                          / cells_intersect_places['area'])\n",
    "cells_counts = cells_intersect_places.groupby('cell_id')['cell_count'].sum()  \n",
    "cell_plot_df = cells_in_area_df.join(cells_counts, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(fig_dir, \n",
    "    'tweet_counts_cc={}_lang={}_cell_size={}m.pdf'.format(cc, plot_lang, cell_size))\n",
    "country_name = shape_df['NAME_ENGL'].values[0]\n",
    "langs_dict = dict(pycld2.LANGUAGES)\n",
    "for key in langs_dict:\n",
    "    if langs_dict[key] == plot_lang:\n",
    "        formatted_lang = key.lower().capitalize()  \n",
    "plot_title = 'Distribution of {} speakers in {}'.format(formatted_lang, country_name)\n",
    "cbar_label = 'Number of tweets in the cell'\n",
    "plot_kwargs = dict(edgecolor='w', linewidths=0.001, cmap='Purples')\n",
    "ax = grid_viz.plot_grid(cell_plot_df, shape_df, metric_col='cell_count', save_path=save_path, \n",
    "                        title=plot_title, cbar_label=cbar_label, **plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_point = shape_df['geometry'].values[0].centroid\n",
    "m = folium.Map(location=[start_point.y, start_point.x], zoom_start=7, tiles='Stamen Toner')\n",
    "folium.Choropleth(\n",
    "    geo_data=cell_plot_df,\n",
    "    name='choropleth',\n",
    "    data=cell_plot_df['cell_count'],\n",
    "    key_on='feature.properties.cell_id',\n",
    "    fill_color='Blues',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=1,\n",
    "    line_color='w',\n",
    "    legend_name=cbar_label\n",
    ").add_to(m)\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = places_counts_df.copy()\n",
    "plot_df['ratio'] = plot_df['count'] / plot_df['count'].max()\n",
    "save_path = os.path.join(fig_dir, \n",
    "    'tweet_counts_cc={}_lang={}_cell_size={}m.pdf'.format(cc, plot_lang, cell_size))\n",
    "plot_kwargs = dict(alpha=0.7, edgecolor='w', legend=True, linewidths=0.001)\n",
    "ax = grid_viz.plot_grid(plot_df, shape_df, metric_col='ratio', save_path=save_path, show=False, **plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIrst tests on single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_year = 2015\n",
    "nr_consec_months = 3\n",
    "tweets_file_path = os.path.join(data_dir_path, tweets_files_format.format(cc))\n",
    "raw_tweets_df_generator = data_access.yield_json(tweets_file_path, \n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, chunk_size=1000000, compression='gzip')\n",
    "agg_tweeted_months_users = pd.DataFrame([], columns=['uid', 'month'])\n",
    "tweets_df_list = []\n",
    "for raw_tweets_df in raw_tweets_df_generator:\n",
    "    tweets_df_list.append(raw_tweets_df)\n",
    "    tweeted_months_users = ufilters.get_months_activity(raw_tweets_df)\n",
    "    agg_tweeted_months_users = pd.concat([agg_tweeted_months_users, tweeted_months_users])\n",
    "raw_tweets_df_generator.close()\n",
    "local_uid_series = ufilters.consec_months(agg_tweeted_months_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_year = 2015\n",
    "nr_consec_months = 3\n",
    "tweeted_months_users = ufilters.get_months_activity(tweets_df)\n",
    "local_uid_series = ufilters.consec_months(tweeted_months_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(local_uid_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets_df['lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets_df.join(local_uid_series, on='uid', how='inner')['lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_path = os.path.join(data_dir_path, tweets_files_format.format(cc))\n",
    "raw_tweets_df_generator = data_access.yield_json(tweets_file_path, \n",
    "    ssh_domain=ssh_domain, ssh_username=ssh_username, chunk_size=1000000, compression='gzip')\n",
    "for raw_tweets_df in raw_tweets_df_generator:\n",
    "    filtered_tweets_df = pd.DataFrame(local_uid_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Languages possibly detected by CLD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_with_code = dict(pycld2.LANGUAGES)\n",
    "detected_lang_with_code = [(lang, lang_with_code[lang]) for lang in pycld2.DETECTED_LANGUAGES]\n",
    "print(detected_lang_with_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Languages possibly detected by Twitter (see 'lang' in https://support.gnip.com/apis/powertrack2.0/rules.html#Operators):\n",
    "\n",
    "Amharic - am\n",
    "Arabic - ar\n",
    "Armenian - hy\n",
    "Bengali - bn\n",
    "Bulgarian - bg\n",
    "Burmese - my\n",
    "Chinese - zh\n",
    "Czech - cs\n",
    "Danish - da\n",
    "Dutch - nl\n",
    "English - en\n",
    "Estonian - et\n",
    "Finnish - fi\n",
    "French - fr\n",
    "Georgian - ka\n",
    "German - de\n",
    "Greek - el\n",
    "Gujarati - gu\n",
    "Haitian - ht\n",
    "Hebrew - iw\n",
    "Hindi - hi\n",
    "Hungarian - hu\n",
    "Icelandic - is\n",
    "Indonesian - in\n",
    "Italian - it\n",
    "Japanese - ja\n",
    "Kannada - kn\n",
    "Khmer - km\n",
    "Korean - ko\n",
    "Lao - lo\n",
    "Latvian - lv\n",
    "Lithuanian - lt\n",
    "Malayalam - ml\n",
    "Maldivian - dv\n",
    "Marathi - mr\n",
    "Nepali - ne\n",
    "Norwegian - no\n",
    "Oriya - or\n",
    "Panjabi - pa\n",
    "Pashto - ps\n",
    "Persian - fa\n",
    "Polish - pl\n",
    "Portuguese - pt\n",
    "Romanian - ro\n",
    "Russian - ru\n",
    "Serbian - sr\n",
    "Sindhi - sd\n",
    "Sinhala - si\n",
    "Slovak - sk\n",
    "Slovenian - sl\n",
    "Sorani Kurdish - ckb\n",
    "Spanish - es\n",
    "Swedish - sv\n",
    "Tagalog - tl\n",
    "Tamil - ta\n",
    "Telugu - te\n",
    "Thai - th\n",
    "Tibetan - bo\n",
    "Turkish - tr\n",
    "Ukrainian - uk\n",
    "Urdu - ur\n",
    "Uyghur - ug\n",
    "Vietnamese - vi\n",
    "Welsh - cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lang_df = text_process.lang_detect(tweets_df, text_col='text', min_nr_words=4, cld='pycld2')\n",
    "tweets_lang_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld_langs = tweets_lang_df['cld_lang'].unique()\n",
    "cld_langs.sort()\n",
    "print('Languages detected by cld: {}'.format(cld_langs))\n",
    "twitter_langs = tweets_lang_df['twitter_lang'].unique()\n",
    "twitter_langs.sort()\n",
    "print('Languages detected by twitter: {}'.format(twitter_langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lang_df['twitter_lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lang_df['cld_lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French case, corsican is unreliably detected by CLD for French tweets, however seems pretty accurate when twitter_lang='it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at multilingual users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_user_lang = tweets_lang_df.loc[tweets_lang_df['twitter_lang'] != 'und'].groupby(['uid', 'twitter_lang'])\n",
    "count_tweets_by_user_lang = groupby_user_lang.size()\n",
    "count_langs_by_user_df = count_tweets_by_user_lang.groupby('uid').transform('size')\n",
    "multiling_users_df = count_tweets_by_user_lang.loc[count_langs_by_user_df > 1]\n",
    "pd.DataFrame(multiling_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)\n",
    "multiling_users_list = [x[0] for x in multiling_users_df.index.values]\n",
    "tweets_lang_df[tweets_lang_df['uid'].isin(multiling_users_list)].sort_values(by=['uid', 'cld_lang'])[\n",
    "    ['uid', 'filtered_text', 'cld_lang', 'twitter_lang', 'created_at']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places into geodf and join on tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the area to discard bbox which are too large? Problem: need to project first, which is expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_loc_df = tweets_lang_df.loc[tweets_lang_df['coordinates'].isnull()]\n",
    "crs = {'init': latlon_proj}\n",
    "places_df = raw_places_df[['id', 'bounding_box', 'name', 'place_type']]\n",
    "geometry = places_df['bounding_box'].apply(lambda x: Polygon(x['coordinates'][0]))\n",
    "places_geodf = geopd.GeoDataFrame(places_df, crs=crs, geometry=geometry)\n",
    "places_geodf = places_geodf.set_index('id')\n",
    "places_geodf = places_geodf.drop(columns=['bounding_box'])\n",
    "places_geodf['area'] = places_geodf.geometry.to_crs(xy_proj).area\n",
    "tweets_final_df = tweets_to_loc_df.join(places_geodf, on='place_id', how='left')\n",
    "tweets_final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final_df.loc[(tweets_final_df['cld_lang'] =='co') & (tweets_final_df['twitter_lang'] =='it')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLD sensitive to letter repetitions made to insist: can put threshold if more than 3 consecutive same letter, bring it down to 2, it seems to improve prediction on example\n",
    "\n",
    "Usually twitter's prediction seems better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final_df[tweets_final_df['cld_lang'] != tweets_final_df['twitter_lang']].drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupbys and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_time(df, dt_col):\n",
    "    t_series_in_sec_of_day = df['hour']*3600 + df['minute']*60 + df['second']\n",
    "    return pd.to_timedelta(int(t_series_in_sec_of_day.mean()), unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = raw_tweets_df.copy()\n",
    "# Speeds up the process to extract the hour, min and sec first\n",
    "tweets_df['hour'] = tweets_df['created_at'].dt.hour\n",
    "tweets_df['minute'] = tweets_df['created_at'].dt.minute\n",
    "tweets_df['second'] = tweets_df['created_at'].dt.second\n",
    "groupby_user_place = tweets_df.groupby(['uid', 'place_id'])\n",
    "count_tweets_by_user_place = groupby_user_place.size()\n",
    "count_tweets_by_user_place.rename('count', inplace=True)\n",
    "mean_time_by_user_place = groupby_user_place.apply(lambda df: get_mean_time(df, 'created_at'))\n",
    "mean_time_by_user_place.rename('avg time', inplace=True)\n",
    "# transform to keep same size, so as to be able to have a matching boolean Series of same size as \n",
    "# original df to select users with more than one place for example:\n",
    "count_places_by_user_df = count_tweets_by_user_place.groupby('uid').transform('size')\n",
    "agg_data_df = pd.concat([count_tweets_by_user_place, mean_time_by_user_place], axis=1)\n",
    "count_tweets_by_user_place_geodf = agg_data_df.join(places_geodf, on='place_id')\n",
    "count_tweets_by_user_place_geodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run(\"groupby_user_place.apply(lambda df: get_mean_time(df, 'created_at'))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweets_by_user_place_geodf.loc[count_places_by_user_df > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new chunk to cumulative data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweets_by_user_place_geodf = count_tweets_by_user_place_geodf.join(\n",
    "    count_tweets_by_user_place_geodf['count'], \n",
    "    on=['uid', 'place_id'], how='outer', rsuffix='_new')\n",
    "count_tweets_by_user_place_geodf['count'] += count_tweets_by_user_place_geodf['count_new']\n",
    "count_tweets_by_user_place_geodf.drop(columns=['count_new'], inplace=True)\n",
    "count_tweets_by_user_place_geodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
